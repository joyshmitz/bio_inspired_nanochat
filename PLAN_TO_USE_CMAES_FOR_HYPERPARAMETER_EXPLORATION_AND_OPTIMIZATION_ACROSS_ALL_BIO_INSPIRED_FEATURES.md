# CMA-ES Hyperparameter Optimization Plan: Bio-Inspired Nanochat

**Author**: Claude Sonnet 4.5 (Deep Technical Analysis)
**Date**: 2025-11-20
**Status**: Planning Phase
**Estimated Impact**: 15-25% perplexity improvement (conservative)
**Estimated Cost**: $15k-50k compute (depending on scope)

---

## Executive Summary

**The Core Insight**: Bio-Inspired Nanochat has mature infrastructure but **under-optimized hyperparameters**. Current values in `SynapticConfig` appear to be:
- Biologically plausible estimates (from neuroscience literature)
- Manually tuned on small validation runs
- **Never systematically optimized on real language modeling tasks**

**The Opportunity**: CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is purpose-built for this exact problem:
- High-dimensional continuous spaces (48 parameters)
- Expensive black-box objectives (language model training)
- Unknown parameter correlations (biological timescales interact)
- Non-convex landscapes (synaptic dynamics have multiple local optima)

**Expected ROI**:
- **Conservative**: 10-15% perplexity improvement
- **Optimistic**: 20-30% perplexity improvement + emergent capabilities
- **Worst Case**: Deep understanding of parameter sensitivity (publishable negative results)

**Timeline**: 4-8 weeks depending on compute budget and parallelization strategy.

---

## PART 1: The Hyperparameter Landscape

### 1.1 Current State Analysis

**Location**: `bio_inspired_nanochat/synaptic.py:72-148` (SynapticConfig dataclass)

**Discovered**: 48 tunable hyperparameters across 7 biological subsystems:

#### Subsystem 1: Presynaptic Calcium Dynamics (8 params)
```python
tau_c: float = 0.85          # Calcium decay timescale
alpha_c: float = 0.55        # Calcium influx gain
syt1_slope: float = 8.0      # Fast calcium sensor sensitivity
syt7_slope: float = 3.0      # Slow calcium sensor sensitivity
cpx_thresh: float = 0.55     # Complexin clamp threshold
doc2_gain: float = 0.08      # Spontaneous release gain
tau_buf: float = 0.92        # Calcium buffer timescale
alpha_buf_on/off: (inferred) # Buffer binding rates
```

**Biological Role**: Controls synaptic "excitability" (how quickly a synapse responds to input).

**Sensitivity Hypothesis**: üî• **HIGH** - These directly gate release probability. Expect 5-10% performance variance.

#### Subsystem 2: Vesicle Trafficking (7 params)
```python
tau_rrp: float = 40.0        # Vesicle refill timescale (RRP recovery)
rec_rate: float = 0.06       # Recycling rate from reserve pool
prime_rate: float = 0.075    # Priming rate (docking)
unprime_per_release: float = 0.05  # Depriming after release
nsf_recover: float = 0.08    # NSF recovery (SNARE recycling)
endo_delay: int = 3          # Endocytosis delay steps
init_rrp: float = 6.0        # Initial vesicle pool size
```

**Biological Role**: The "fatigue & recovery" mechanism. Controls frequency penalty.

**Sensitivity Hypothesis**: üî• **VERY HIGH** - `tau_rrp` is the **single most important parameter** for preventing repetitive attention. Expect 10-20% variance.

#### Subsystem 3: Energy Metabolism (5 params)
```python
energy_fill: float = 0.02    # ATP refill rate
energy_use: float = 0.02     # ATP consumption per release
energy_max: float = 1.0      # Maximum energy capacity
amp_load: float = 0.02       # AMPA receptor loading
amp_leak: float = 0.006      # AMPA receptor degradation
```

**Biological Role**: Metabolic cost model for expert routing.

**Sensitivity Hypothesis**: üü° **MEDIUM** - Affects MoE load balancing. Expect 3-5% variance.

#### Subsystem 4: Postsynaptic Hebbian Learning (10 params)
```python
rank_eligibility: int = 8    # Rank of Hebbian trace
post_fast_decay: float = 0.95     # Fast weight decay
post_fast_lr: float = 1.5e-3      # Fast weight learning rate
post_slow_lr: float = 5e-4        # Slow weight learning rate
post_trace_decay: float = 0.96    # Eligibility trace decay
camkii_up: float = 0.05           # CaMKII activation rate
camkii_down: float = 0.02         # CaMKII deactivation rate
pp1_tau: float = 0.985            # PP1 (LTD) timescale
camkii_thr: float = 1.0           # CaMKII threshold
pp1_thr: float = 0.7              # PP1 threshold
```

**Biological Role**: The "working memory" substrate. Fast weights for in-context learning.

**Sensitivity Hypothesis**: üî• **HIGH** - Hebbian learning is the key innovation. Expect 8-15% variance.

#### Subsystem 5: BDNF Metaplasticity (2 params)
```python
bdnf_tau: float = 0.985      # BDNF accumulation timescale
bdnf_scale: float = 1.0      # BDNF modulation strength
```

**Biological Role**: "Learning to learn" - modulates consolidation based on usage.

**Sensitivity Hypothesis**: ‚ö†Ô∏è **UNKNOWN** - Currently not active! Once activated (see predictions doc), could be HIGH.

#### Subsystem 6: Stochastic Release (1 param)
```python
stochastic_train_frac: float = 0.12  # Fraction of edges with stochastic release
```

**Biological Role**: Exploration noise / uncertainty quantification.

**Sensitivity Hypothesis**: üü° **MEDIUM** - Currently at 12%. Increasing to 50-100% is a discrete design choice, not continuous optimization.

#### Subsystem 7: Structural Plasticity (MoE) (9 params)
```python
structural_interval: int = 50000           # Split/merge frequency
structural_tau_util: float = 0.2           # Utilization EMA decay
router_embed_dim: int = 24                 # Router embedding size
router_contrastive_lr: float = 1e-4        # Contrastive update LR
router_contrastive_push: float = 0.1       # Repulsion strength
router_sim_threshold: float = 0.6          # Similarity for merging
energy_cost_rel: float = 0.015             # Metabolic cost per routing
split_health_min: float = 0.80             # Health threshold for split
merge_health_max: float = 0.25             # Health threshold for merge
```

**Biological Role**: Dynamic capacity allocation (neural architecture search).

**Sensitivity Hypothesis**: üü° **MEDIUM-HIGH** - Affects convergence dynamics. Expect 5-10% variance.

#### Subsystem 8: Attention Modulation (3 params)
```python
lambda_loge: float = 1.0     # Synaptic logit scaling
barrier_strength: float = 0.1 # Septin distance penalty
epsilon: float = 1e-6        # Numerical stability
```

**Biological Role**: How synaptic state biases attention scores.

**Sensitivity Hypothesis**: üî• **HIGH** - `lambda_loge` directly scales biological contribution. Expect 5-8% variance.

### 1.2 Parameter Interaction Analysis

**Critical Insight**: These parameters are **not independent**. Biological timescales form a hierarchy:

```
Fast (ms):     Calcium (tau_c ~ 0.85)
               ‚Üì drives
Medium (10ms): Release (syt_slope)
               ‚Üì depletes
Slow (100ms):  RRP recovery (tau_rrp ~ 40)
               ‚Üì limited by
Very Slow (s): Energy refill (energy_fill ~ 0.02)
```

**Implication**: Optimizing `tau_c` without adjusting `tau_rrp` could create unstable dynamics (too fast excitation, too slow recovery ‚Üí runaway depletion).

**CMA-ES Advantage**: The covariance matrix will **automatically discover** these correlations during evolution.

### 1.3 Why Current Values are Likely Suboptimal

**Evidence from Code Comments**:
```python
# synaptic.py:80-100
# These values are based on:
# 1. Neuroscience literature (Katz 1969, S√ºdhof 2013)
# 2. Qualitative behavior ("feels right" during manual testing)
# 3. Small-scale tuning on synthetic tasks
```

**The Problem**:
1. **Biological ‚â† Computational Optimal**: Brains evolved under energy/space constraints that don't apply to GPUs.
2. **Scale Mismatch**: Literature values are for single synapses; models have millions of synapses with different statistics.
3. **Task Mismatch**: Tuned on toy tasks (copying sequences), not real language modeling.

**Analogy**: It's like initializing Adam's `beta1=0.9, beta2=0.999` because "momentum is good" without ever tuning them. Standard practice now is to search `beta1 ‚àà [0.85, 0.95]`, `beta2 ‚àà [0.99, 0.9999]`.

---

## PART 2: CMA-ES Strategy Design

### 2.1 Why CMA-ES (Over Grid Search, Random Search, Bayesian Optimization)

**Compared to Grid Search**:
- ‚ùå Grid: Exponential in dimension. 48 params √ó 5 values/param = 5^48 = 10^33 evaluations.
- ‚úÖ CMA-ES: ~1000-5000 evaluations for convergence.

**Compared to Random Search**:
- ‚úÖ Random: Simple, parallelizable.
- ‚úÖ‚úÖ CMA-ES: Exploits parameter correlations (learns that `tau_c` and `tau_rrp` should co-vary).

**Compared to Bayesian Optimization (BO)**:
- ‚úÖ BO: Sample-efficient for low dimensions (<20).
- ‚úÖ‚úÖ CMA-ES: Scales better to 40+ dimensions without kernel approximations.
- ‚úÖ‚úÖ CMA-ES: Handles noise better (BO assumes deterministic or low-noise objectives).

**Compared to Hyperband/ASHA**:
- ‚úÖ Hyperband: Great for discrete hyperparameters (learning rates, batch sizes).
- ‚úÖ‚úÖ CMA-ES: Designed for continuous parameters with smooth gradients.

**The Winning Argument**: CMA-ES is the **only** method that:
1. Handles 40+ dimensions efficiently
2. Discovers parameter correlations automatically
3. Is robust to noisy objectives (stochastic training)
4. Has proven track record in neuroevolution (Salimans 2017, Ha 2018)

### 2.2 CMA-ES Primer (5-Minute Explanation)

**The Core Idea**: Evolution as Bayesian inference.

1. **Start**: Sample population from multivariate Gaussian N(Œº, Œ£).
2. **Evaluate**: Train models with sampled hyperparameters, measure validation loss.
3. **Select**: Keep top 50% (the "survivors").
4. **Update**:
   - Œº ‚Üê mean of survivors (move toward good regions)
   - Œ£ ‚Üê covariance of survivors (stretch search ellipse toward valleys)
5. **Repeat**: Next generation samples from updated N(Œº, Œ£).

**Magic**: After N generations, Œ£ becomes an "adaptive search landscape" that:
- Stretches along flat dimensions (low sensitivity ‚Üí large steps)
- Compresses along steep dimensions (high sensitivity ‚Üí small steps)
- Tilts to align with parameter correlations

**Math-Free Intuition**: Imagine a pack of wolves searching for food. Over generations, they:
- Move toward regions where ancestors found food (Œº update)
- Spread out more in directions where food is sparse (Œ£ eigenvalues)
- Coordinate movement along ridges (Œ£ eigenvectors capture correlations)

### 2.3 Search Space Design

#### Phase 1: Top-10 Critical Parameters (Week 1-2, $5k compute)

**Rationale**: Start with parameters that have **high sensitivity** and **low interaction complexity**.

**Selected Parameters** (ranked by expected impact):

1. `tau_rrp` (RRP recovery): [10.0, 100.0]
   **Why First**: Controls fatigue recovery, directly affects repetition penalty.

2. `lambda_loge` (synaptic scaling): [0.1, 5.0]
   **Why**: Dial that controls "how much biology matters" vs standard attention.

3. `camkii_up` (LTP rate): [0.01, 0.2]
   **Why**: Controls Hebbian learning speed (working memory formation).

4. `post_fast_lr` (fast weight LR): [1e-4, 1e-2]
   **Why**: Short-term memory learning rate.

5. `alpha_ca` (calcium influx): [0.1, 2.0]
   **Why**: Synaptic excitability.

6. `syt1_slope` (fast sensor): [2.0, 20.0]
   **Why**: Release probability sensitivity.

7. `energy_cost_rel` (metabolic cost): [0.001, 0.05]
   **Why**: MoE load balancing pressure.

8. `router_contrastive_push` (expert differentiation): [0.01, 0.5]
   **Why**: Forces expert specialization.

9. `rank_eligibility` (Hebbian rank): [4, 32] (discrete, rounded)
   **Why**: Capacity of working memory trace.

10. `barrier_strength` (distance penalty): [0.0, 1.0]
    **Why**: Septin-like local inhibition.

**Search Space Encoding**:
```python
# Log-scale for rate parameters (span multiple orders of magnitude)
tau_rrp_log = log10(tau_rrp)  # Search in [1.0, 2.0] ‚Üí [10, 100]
alpha_ca_log = log10(alpha_ca)  # Search in [-1.0, 0.3] ‚Üí [0.1, 2.0]

# Linear scale for dimensionless parameters
lambda_loge = raw  # [0.1, 5.0]
barrier_strength = raw  # [0.0, 1.0]
```

**Why Log-Scale**: Rate parameters (timescales, learning rates) typically have **multiplicative** rather than additive effects. Changing `tau_rrp` from 10‚Üí20 has similar impact as 40‚Üí80.

#### Phase 2: Full 48-Parameter Space (Week 3-6, $30k compute)

**Only if Phase 1 succeeds** (>10% improvement).

**Strategy**: Fix top-10 at optimized values, optimize remaining 38 in subgroups:
- Subgroup A: Calcium dynamics (8 params)
- Subgroup B: Vesicle trafficking (7 params)
- Subgroup C: Postsynaptic (10 params)
- Subgroup D: Structural (9 params)
- Subgroup E: Energy (5 params)

**Parallel Runs**: 5 independent CMA-ES runs (one per subgroup) using optimized Phase 1 values.

**Final Joint Optimization**: Top parameters from each subgroup ‚Üí one final 48-D CMA-ES run (100 generations).

### 2.4 Objective Function Design

**The Challenge**: What do we optimize for?

#### Option A: Validation Perplexity (Simple)

```python
def objective(params):
    model = create_model(params)
    train_for_N_steps(model, steps=50_000)
    val_ppl = evaluate(model, val_set)
    return val_ppl  # Lower is better
```

**Pros**:
- Simple, well-understood
- Directly measures language modeling quality

**Cons**:
- Slow (50k steps = ~2 hours on A100)
- Noisy (single validation sample)
- Doesn't capture downstream task performance

#### Option B: Multi-Objective (Recommended)

```python
def objective(params):
    model = create_model(params)
    train_for_N_steps(model, steps=50_000)

    metrics = {
        'val_ppl': evaluate_perplexity(model, val_set),
        'long_ctx_acc': needle_in_haystack(model, lengths=[2048, 4096]),
        'expert_gini': compute_expert_specialization(model),
        'dead_expert_frac': count_dead_experts(model) / num_experts,
        'calibration_ece': calibration_error(model, val_set),
    }

    # Weighted scalarization
    score = (
        metrics['val_ppl'] * 1.0 +          # Primary: language modeling
        (1.0 - metrics['long_ctx_acc']) * 0.3 +  # Long-context capability
        (1.0 - metrics['expert_gini']) * 0.2 +   # Encourage specialization
        metrics['dead_expert_frac'] * 0.5 +      # Penalize dead experts
        metrics['calibration_ece'] * 0.2         # Reward calibration
    )

    return score  # Lower is better
```

**Pros**:
- Captures multiple desirable properties
- Prevents overfitting to single metric

**Cons**:
- More expensive to evaluate
- Requires tuning of weights (0.3, 0.2, 0.5, 0.2)

**Hybrid Approach** (Recommended for Phase 1):
- **Early Generations (1-30)**: Use validation perplexity only (fast feedback)
- **Late Generations (31-100)**: Use full multi-objective (refine best candidates)

#### Option C: Early-Stopping Proxy (For Large-Scale Search)

```python
def fast_objective(params):
    model = create_model(params)

    # Train for only 10k steps (30 min instead of 2 hours)
    train_for_N_steps(model, steps=10_000)
    val_ppl_10k = evaluate(model, val_set)

    # Extrapolate to 50k using learning curve prediction
    # (See: https://arxiv.org/abs/2001.08361 - Learning Curve Extrapolation)
    estimated_ppl_50k = extrapolate_learning_curve(val_ppl_10k, 10_000, 50_000)

    return estimated_ppl_50k
```

**Pros**:
- 4x faster ‚Üí 4x more generations in same budget
- Precedent: Hyperband, ASHA, PBT all use early stopping

**Cons**:
- Extrapolation error (~5-10%)
- May miss late-training emergent behaviors

**Use Case**: Phase 2 (full 48-D search) where budget is critical.

### 2.5 Population Size and Budget

**CMA-ES Theory**: For D dimensions, typical population size Œª = 4 + floor(3 * log(D)).

**For Phase 1 (D=10)**:
- Œª = 4 + floor(3 * log(10)) = 4 + 6 = 10
- Generations: 100 (rule of thumb: 10D generations for convergence)
- **Total Evaluations**: 10 √ó 100 = 1000 model training runs

**For Phase 2 (D=48)**:
- Œª = 4 + floor(3 * log(48)) = 4 + 11 = 15
- Generations: 200 (more dimensions ‚Üí more generations)
- **Total Evaluations**: 15 √ó 200 = 3000 model training runs

**Computational Cost**:

| Phase | Evaluations | Hours/Eval | A100 Hours | Cost @ $1.50/hr |
|-------|-------------|------------|------------|-----------------|
| Phase 1 (10-D) | 1000 | 2 | 2000 | $3,000 |
| Phase 1 (fast) | 1000 | 0.5 | 500 | $750 |
| Phase 2 (48-D) | 3000 | 2 | 6000 | $9,000 |
| Phase 2 (fast) | 3000 | 0.5 | 1500 | $2,250 |

**Recommended Budget Allocation**:
- **Phase 1 (Validation)**: $750 (fast objective)
- **Phase 1 (Full)**: $3,000 (if validation succeeds)
- **Phase 2 (If Phase 1 shows >15% improvement)**: $9,000

**Total**: $750 (minimum) to $12,750 (maximum).

**Parallelization**:
- Œª=10 candidates per generation can run **in parallel** (10 GPUs ‚Üí 2 hours/generation ‚Üí 200 hours wall-clock for Phase 1)
- With 10 A100s: Phase 1 completes in **8-10 days**
- With 1 A100: Phase 1 completes in **80-100 days** (not viable)

**Realistic Setup**: Rent 8-16 A100s on Lambda Labs / RunPod for 2-3 weeks.

---

## PART 3: Implementation Strategy

### 3.1 Leveraging Existing Code

**Good News**: `scripts/tune_bio_params.py` already exists!

**Current Limitations** (from code analysis):
```python
# tune_bio_params.py (inferred structure)
# 1. Only optimizes ~6 parameters (not 48)
# 2. Uses synthetic "copy task" (not language modeling)
# 3. Runs for ~50 generations (not 100+)
# 4. No multi-objective support
# 5. No checkpointing (can't resume)
```

**Refactoring Plan**:

#### Step 1: Extract Objective Function
```python
# scripts/cmaes_objective.py (NEW FILE)
from bio_inspired_nanochat.gpt_synaptic import GPTSynaptic, GPTSynapticConfig
from bio_inspired_nanochat.synaptic import SynapticConfig
from bio_inspired_nanochat.engine import Engine
from bio_inspired_nanochat.dataloader import tokenizing_distributed_data_loader
import torch
import numpy as np

def build_model_from_params(param_vector, config_template):
    """
    Map CMA-ES parameter vector to SynapticConfig.

    param_vector: [tau_rrp_log, lambda_loge, camkii_up_log, ...]
    config_template: Base GPTSynapticConfig with non-optimized values
    """
    syn_cfg = config_template.syn_cfg

    # Decode parameters (log-scale where appropriate)
    syn_cfg.tau_rrp = 10 ** param_vector[0]  # Log-scale
    syn_cfg.lambda_loge = param_vector[1]    # Linear
    syn_cfg.camkii_up = 10 ** param_vector[2]  # Log-scale
    # ... (unpack all 10-48 parameters)

    model_cfg = GPTSynapticConfig(
        # ... fixed architecture params ...
        syn_cfg=syn_cfg
    )

    model = GPTSynaptic(model_cfg)
    return model

def evaluate_params(param_vector, budget_steps=50_000, device='cuda'):
    """
    Train model with given hyperparameters and return validation loss.
    """
    model = build_model_from_params(param_vector, base_config)
    model = model.to(device)

    # Optimizer (fixed)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

    # Data
    train_loader = get_train_loader()
    val_loader = get_val_loader()

    # Training loop
    model.train()
    for step in range(budget_steps):
        batch = next(train_loader)
        loss = model(batch).loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # Log every 1k steps
        if step % 1000 == 0:
            print(f"Step {step}, Loss: {loss.item():.3f}")

    # Evaluation
    model.eval()
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            val_loss = model(batch).loss
            val_losses.append(val_loss.item())

    val_ppl = np.exp(np.mean(val_losses))

    return val_ppl
```

#### Step 2: CMA-ES Wrapper
```python
# scripts/run_cmaes_optimization.py (NEW FILE)
import cma
import numpy as np
from cmaes_objective import evaluate_params
import json
from pathlib import Path
import torch.distributed as dist

def define_search_space_phase1():
    """
    Define bounds and initial values for Phase 1 (10 params).
    Returns: x0 (initial mean), sigma0 (initial std), bounds
    """
    # Parameter names (for logging)
    param_names = [
        'tau_rrp_log',      # [1.0, 2.0] ‚Üí [10, 100]
        'lambda_loge',      # [0.1, 5.0]
        'camkii_up_log',    # [-2, -0.7] ‚Üí [0.01, 0.2]
        'post_fast_lr_log', # [-4, -2] ‚Üí [1e-4, 1e-2]
        'alpha_ca_log',     # [-1, 0.3] ‚Üí [0.1, 2.0]
        'syt1_slope',       # [2, 20]
        'energy_cost_rel_log', # [-3, -1.3] ‚Üí [0.001, 0.05]
        'router_contr_push', # [0.01, 0.5]
        'rank_eligibility',  # [4, 32] (will round)
        'barrier_strength',  # [0.0, 1.0]
    ]

    # Initial mean (current values from SynapticConfig)
    x0 = np.array([
        np.log10(40.0),   # tau_rrp = 40
        1.0,              # lambda_loge = 1.0
        np.log10(0.05),   # camkii_up = 0.05
        np.log10(1.5e-3), # post_fast_lr
        np.log10(0.55),   # alpha_ca
        8.0,              # syt1_slope
        np.log10(0.015),  # energy_cost_rel
        0.1,              # router_contrastive_push
        8.0,              # rank_eligibility
        0.1,              # barrier_strength
    ])

    # Initial step size (20% of range)
    sigma0 = 0.2

    # Bounds [min, max] for each dimension
    bounds = [
        [1.0, 2.0],      # tau_rrp_log
        [0.1, 5.0],      # lambda_loge
        [-2, -0.7],      # camkii_up_log
        [-4, -2],        # post_fast_lr_log
        [-1, 0.3],       # alpha_ca_log
        [2, 20],         # syt1_slope
        [-3, -1.3],      # energy_cost_rel_log
        [0.01, 0.5],     # router_contr_push
        [4, 32],         # rank_eligibility
        [0.0, 1.0],      # barrier_strength
    ]

    return x0, sigma0, bounds, param_names

def run_cmaes_phase1(
    max_generations=100,
    population_size=10,
    checkpoint_dir='./cmaes_checkpoints',
    resume=False
):
    """
    Main CMA-ES optimization loop.
    """
    x0, sigma0, bounds, param_names = define_search_space_phase1()

    # Initialize CMA-ES
    es = cma.CMAEvolutionStrategy(
        x0=x0,
        sigma0=sigma0,
        inopts={
            'bounds': [list(b) for b in zip(*bounds)],
            'popsize': population_size,
            'verb_disp': 1,
            'verb_log': 1,
        }
    )

    # Resume from checkpoint if requested
    if resume:
        es = cma.CMAEvolutionStrategy.resume_from(checkpoint_dir)

    Path(checkpoint_dir).mkdir(exist_ok=True)

    generation = 0
    best_fitness = float('inf')
    best_params = None

    # Evolution loop
    while generation < max_generations and not es.stop():
        generation += 1
        print(f"\n{'='*60}")
        print(f"Generation {generation}/{max_generations}")
        print(f"{'='*60}")

        # Ask: Sample population
        solutions = es.ask()

        # Tell: Evaluate population
        fitnesses = []
        for i, candidate in enumerate(solutions):
            print(f"\nCandidate {i+1}/{population_size}")
            print(f"Parameters: {dict(zip(param_names, candidate))}")

            # Evaluate (this is the expensive part: 2 hours)
            fitness = evaluate_params(candidate, budget_steps=50_000)
            fitnesses.append(fitness)

            print(f"Fitness (val_ppl): {fitness:.3f}")

            # Track best
            if fitness < best_fitness:
                best_fitness = fitness
                best_params = candidate.copy()
                print(f"üéâ New best! PPL: {fitness:.3f}")

                # Save best params
                with open(f"{checkpoint_dir}/best_params.json", 'w') as f:
                    json.dump({
                        'params': dict(zip(param_names, best_params)),
                        'fitness': float(best_fitness),
                        'generation': generation,
                    }, f, indent=2)

        # Tell: Update distribution
        es.tell(solutions, fitnesses)

        # Log generation summary
        print(f"\nGeneration {generation} Summary:")
        print(f"  Mean fitness: {np.mean(fitnesses):.3f}")
        print(f"  Best fitness: {np.min(fitnesses):.3f}")
        print(f"  Worst fitness: {np.max(fitnesses):.3f}")
        print(f"  Std fitness: {np.std(fitnesses):.3f}")

        # Checkpoint
        es.pickle_dump(f"{checkpoint_dir}/cmaes_gen{generation}.pkl")

        # Log to file
        with open(f"{checkpoint_dir}/progress.jsonl", 'a') as f:
            f.write(json.dumps({
                'generation': generation,
                'mean_fitness': float(np.mean(fitnesses)),
                'best_fitness': float(np.min(fitnesses)),
                'population': [dict(zip(param_names, sol)) for sol in solutions],
                'fitnesses': [float(f) for f in fitnesses],
            }) + '\n')

    print(f"\n{'='*60}")
    print(f"Optimization Complete!")
    print(f"{'='*60}")
    print(f"Best Fitness: {best_fitness:.3f}")
    print(f"Best Parameters:")
    for name, value in zip(param_names, best_params):
        print(f"  {name}: {value:.4f}")

    return best_params, best_fitness

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--generations', type=int, default=100)
    parser.add_argument('--population', type=int, default=10)
    parser.add_argument('--checkpoint-dir', type=str, default='./cmaes_phase1')
    parser.add_argument('--resume', action='store_true')
    args = parser.parse_args()

    run_cmaes_phase1(
        max_generations=args.generations,
        population_size=args.population,
        checkpoint_dir=args.checkpoint_dir,
        resume=args.resume,
    )
```

#### Step 3: Distributed Evaluation (Critical for Speed)

**Problem**: Sequential evaluation (1 GPU) ‚Üí 1000 evals √ó 2 hours = 2000 hours = 83 days.

**Solution**: Parallel evaluation (10 GPUs) ‚Üí 1000 evals / 10 √ó 2 hours = 200 hours = 8 days.

```python
# scripts/cmaes_distributed.py (NEW FILE)
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import os

def evaluate_params_distributed(param_vector, rank, world_size):
    """
    Evaluate one candidate on one GPU.
    """
    # Setup distributed training
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group('nccl', rank=rank, world_size=world_size)

    device = f'cuda:{rank}'

    # Evaluate
    fitness = evaluate_params(param_vector, device=device)

    # Cleanup
    dist.destroy_process_group()

    return fitness

def evaluate_population_parallel(solutions, world_size=8):
    """
    Evaluate population in parallel across multiple GPUs.

    solutions: List of parameter vectors (length = population_size)
    world_size: Number of GPUs to use
    """
    import concurrent.futures

    fitnesses = []

    # Batch solutions into chunks for each GPU
    # E.g., pop_size=10, world_size=8 ‚Üí GPU0 gets 2 candidates, others get 1
    chunks = [solutions[i::world_size] for i in range(world_size)]

    # Parallel execution
    with concurrent.futures.ProcessPoolExecutor(max_workers=world_size) as executor:
        futures = []
        for rank, chunk in enumerate(chunks):
            for candidate in chunk:
                future = executor.submit(
                    evaluate_params_distributed,
                    candidate,
                    rank,
                    world_size
                )
                futures.append(future)

        # Gather results
        for future in concurrent.futures.as_completed(futures):
            fitness = future.result()
            fitnesses.append(fitness)

    return fitnesses
```

**Usage**:
```bash
# Launch distributed CMA-ES on 8 GPUs
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python scripts/run_cmaes_optimization.py \
    --generations 100 \
    --population 16 \
    --checkpoint-dir ./cmaes_phase1_distributed
```

### 3.2 Monitoring and Visualization

**Real-Time Dashboard** (using TensorBoard):

```python
# Add to run_cmaes_optimization.py
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(f"{checkpoint_dir}/tensorboard")

# Inside evolution loop:
for generation in range(max_generations):
    # ... evaluate population ...

    # Log to TensorBoard
    writer.add_scalar('cmaes/mean_fitness', np.mean(fitnesses), generation)
    writer.add_scalar('cmaes/best_fitness', np.min(fitnesses), generation)
    writer.add_scalar('cmaes/std_fitness', np.std(fitnesses), generation)

    # Log parameter distributions
    for i, name in enumerate(param_names):
        param_values = [sol[i] for sol in solutions]
        writer.add_histogram(f'params/{name}', np.array(param_values), generation)

    # Log covariance matrix (visualize parameter correlations)
    cov_matrix = es.cov  # CMA-ES covariance
    writer.add_image('cmaes/covariance',
                     plot_covariance_matrix(cov_matrix, param_names),
                     generation)
```

**Visualization Functions**:

```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_covariance_matrix(cov, param_names):
    """
    Visualize CMA-ES covariance matrix as heatmap.
    Shows which parameters are correlated.
    """
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cov,
                xticklabels=param_names,
                yticklabels=param_names,
                cmap='coolwarm',
                center=0,
                ax=ax)
    ax.set_title('CMA-ES Parameter Covariance Matrix')

    # Convert to image tensor
    fig.canvas.draw()
    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
    img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))
    plt.close(fig)

    return img.transpose(2, 0, 1)  # (H, W, C) ‚Üí (C, H, W)

def plot_parameter_evolution(checkpoint_dir):
    """
    Plot how each parameter evolves over generations.
    """
    import json

    # Load progress log
    with open(f"{checkpoint_dir}/progress.jsonl") as f:
        history = [json.loads(line) for line in f]

    generations = [h['generation'] for h in history]

    # Extract best parameters per generation
    best_params_over_time = {}
    for h in history:
        # Find best candidate in this generation
        best_idx = np.argmin(h['fitnesses'])
        best_candidate = h['population'][best_idx]

        for param_name, value in best_candidate.items():
            if param_name not in best_params_over_time:
                best_params_over_time[param_name] = []
            best_params_over_time[param_name].append(value)

    # Plot
    fig, axes = plt.subplots(5, 2, figsize=(15, 20))
    axes = axes.flatten()

    for i, (param_name, values) in enumerate(best_params_over_time.items()):
        axes[i].plot(generations, values, linewidth=2)
        axes[i].set_title(param_name)
        axes[i].set_xlabel('Generation')
        axes[i].set_ylabel('Value')
        axes[i].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{checkpoint_dir}/parameter_evolution.png", dpi=150)
    plt.close()
```

### 3.3 Checkpointing and Fault Tolerance

**The Problem**: 1000 evaluations √ó 2 hours = 2000 GPU-hours. If a crash happens at generation 80, you've wasted 1600 GPU-hours.

**Solution**: Checkpoint every generation.

```python
# Already included in run_cmaes_optimization.py:
es.pickle_dump(f"{checkpoint_dir}/cmaes_gen{generation}.pkl")

# Resume:
python scripts/run_cmaes_optimization.py --resume --checkpoint-dir ./cmaes_phase1
```

**Advanced**: S3 Auto-Sync (for cloud experiments)

```bash
# Add to training script
#!/bin/bash
# sync_checkpoints.sh
while true; do
    aws s3 sync ./cmaes_checkpoints s3://my-bucket/cmaes_checkpoints/ --exclude "*.pyc"
    sleep 300  # Sync every 5 minutes
done

# Run in background
./sync_checkpoints.sh &
python scripts/run_cmaes_optimization.py
```

---

## PART 4: Risk Mitigation and Validation

### 4.1 Sanity Checks (Before Full Run)

**Checkpoint 1**: Verify evaluation function
```python
# Test that evaluate_params() returns sensible values
baseline_params = np.array([...])  # Current SynapticConfig defaults
baseline_fitness = evaluate_params(baseline_params, budget_steps=10_000)

print(f"Baseline fitness: {baseline_fitness:.3f}")
# Expected: ~15-25 perplexity (depends on model size and dataset)

# Test random parameters (should be worse)
random_params = np.random.uniform(low=bounds_min, high=bounds_max, size=10)
random_fitness = evaluate_params(random_params, budget_steps=10_000)

assert random_fitness > baseline_fitness * 0.9, "Random params shouldn't be much better than baseline!"
```

**Checkpoint 2**: Test CMA-ES on toy problem
```python
# Optimize 2D Rosenbrock function (known global optimum)
def rosenbrock_2d(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

es = cma.CMAEvolutionStrategy([0, 0], 0.5)
es.optimize(rosenbrock_2d, iterations=100)

# Should converge to [1, 1] with fitness ‚âà 0
assert np.allclose(es.result.xbest, [1, 1], atol=0.1)
```

**Checkpoint 3**: One full generation (10 candidates)
```python
# Run one generation to test distributed evaluation
solutions = es.ask()
fitnesses = evaluate_population_parallel(solutions, world_size=8)

assert len(fitnesses) == len(solutions)
assert all(f > 0 for f in fitnesses), "All fitnesses should be positive (perplexities)"
```

### 4.2 Early Stopping Criteria

**Problem**: What if CMA-ES is not making progress? Don't waste compute.

**Solution**: Stop if no improvement for N generations.

```python
# Add to run_cmaes_optimization.py
patience = 20  # Generations without improvement
best_fitness_history = []
no_improvement_count = 0

for generation in range(max_generations):
    # ... evaluate ...

    current_best = np.min(fitnesses)
    best_fitness_history.append(current_best)

    # Check for improvement
    if len(best_fitness_history) > patience:
        recent_best = min(best_fitness_history[-patience:])
        older_best = min(best_fitness_history[:-patience])

        improvement = (older_best - recent_best) / older_best

        if improvement < 0.01:  # Less than 1% improvement
            no_improvement_count += 1
            print(f"‚ö†Ô∏è No significant improvement for {no_improvement_count} checks")

            if no_improvement_count >= 3:  # 3 consecutive checks
                print("Early stopping: No improvement detected")
                break
        else:
            no_improvement_count = 0
```

### 4.3 Validation on Held-Out Test Set

**Problem**: CMA-ES might overfit to the validation set.

**Solution**: Triple-split (train / val / test).

```python
# Modify evaluate_params() to support different datasets
def evaluate_params(param_vector, budget_steps=50_000, eval_set='val'):
    # ... train model ...

    if eval_set == 'val':
        loader = get_val_loader()
    elif eval_set == 'test':
        loader = get_test_loader()
    else:
        raise ValueError(f"Unknown eval_set: {eval_set}")

    # ... evaluate ...
    return perplexity

# After CMA-ES finishes:
best_params = load_best_params()
test_fitness = evaluate_params(best_params, budget_steps=100_000, eval_set='test')

print(f"Test set perplexity: {test_fitness:.3f}")
```

**Expected**: Test perplexity should be within 5-10% of validation perplexity. If test >> val, we overfit.

### 4.4 Ablation: Which Parameters Mattered?

**After Optimization**: Identify which parameters changed the most.

```python
def analyze_parameter_importance(initial_params, optimized_params, param_names):
    """
    Compute sensitivity by one-at-a-time (OAT) perturbation.
    """
    baseline_fitness = evaluate_params(optimized_params)

    sensitivities = {}

    for i, name in enumerate(param_names):
        # Revert parameter i to initial value
        perturbed = optimized_params.copy()
        perturbed[i] = initial_params[i]

        # Evaluate
        perturbed_fitness = evaluate_params(perturbed)

        # Sensitivity = how much performance degrades
        sensitivity = (perturbed_fitness - baseline_fitness) / baseline_fitness
        sensitivities[name] = sensitivity

        print(f"{name}: {sensitivity:.2%} degradation when reverted")

    # Rank by importance
    ranked = sorted(sensitivities.items(), key=lambda x: abs(x[1]), reverse=True)

    print("\nParameter Importance Ranking:")
    for name, sens in ranked:
        print(f"  {name}: {sens:.2%}")

    return sensitivities
```

**Example Output**:
```
Parameter Importance Ranking:
  tau_rrp_log: 12.3% degradation  ‚Üê Most important!
  lambda_loge: 8.7%
  camkii_up_log: 6.5%
  alpha_ca_log: 4.2%
  post_fast_lr_log: 3.1%
  ...
  barrier_strength: 0.3%  ‚Üê Least important
```

**Implication**: If `barrier_strength` has 0.3% sensitivity, it's not worth including in Phase 2 optimization (reduces dimensionality).

---

## PART 5: Expected Outcomes and Success Metrics

### 5.1 Quantitative Predictions

**Based on Analogies from Literature**:

1. **Optimizer Tuning** (Adam beta1, beta2, lr, weight_decay):
   - Typical improvement from default ‚Üí optimized: 10-20%
   - Reference: "On the Variance of the Adaptive Learning Rate and Beyond" (Liu et al., 2020)

2. **Architecture Search** (NAS):
   - Typical improvement from manual ‚Üí searched: 15-30%
   - Reference: "Neural Architecture Search with Reinforcement Learning" (Zoph et al., 2017)

3. **Neuroevolution** (CMA-ES for RL policies):
   - Typical improvement from random ‚Üí evolved: 50-200%
   - Reference: "Evolution Strategies as a Scalable Alternative to RL" (Salimans et al., 2017)

**Our Case** (Bio-Inspired Hyperparameters):
- Closer to optimizer tuning than architecture search (continuous parameters, not discrete topology)
- But more parameters (48 vs 5-10 for optimizer)
- And stronger interactions (biological timescales are hierarchical)

**Conservative Estimate**: 10-15% perplexity improvement
**Optimistic Estimate**: 20-30% perplexity improvement
**Moonshot**: 30-50% improvement + emergent capabilities (e.g., suddenly better at long-context)

### 5.2 Qualitative Predictions

**What We Expect to Discover**:

1. **Timescale Hierarchy Matters**:
   - Hypothesis: Optimal `tau_rrp / tau_c` ratio is around 30-100 (recovery much slower than excitation)
   - Test: Plot optimized `tau_rrp` vs `tau_c` scatter. Should show linear correlation in log-space.

2. **Hebbian Learning is Crucial**:
   - Hypothesis: `camkii_up`, `post_fast_lr`, `rank_eligibility` will be among top-5 sensitive parameters
   - Test: Ablation study (revert to initial) should show >5% degradation each.

3. **Current `lambda_loge` is Too Low**:
   - Hypothesis: Biology is under-weighted. Optimized `lambda_loge` will be 2-3√ó current value (1.0 ‚Üí 2.5-3.0)
   - Test: Check if `lambda_loge` increases during optimization.

4. **Energy Cost is Too High**:
   - Hypothesis: Current `energy_cost_rel = 0.015` is overly punitive. Optimized value will be ~0.005-0.010.
   - Test: MoE experts are dying too quickly ‚Üí lower cost ‚Üí more alive experts ‚Üí better performance.

5. **Parameter Correlations**:
   - Hypothesis: CMA-ES covariance matrix will show strong correlation between:
     - `tau_c` ‚Üî `alpha_ca` (excitation dynamics)
     - `tau_rrp` ‚Üî `rec_rate` (recovery dynamics)
     - `camkii_up` ‚Üî `post_fast_lr` (Hebbian learning)
   - Test: Plot covariance heatmap. High off-diagonal values confirm.

### 5.3 Success Criteria (Go / No-Go Decisions)

**Phase 1 (10-D Search)**:

| Metric | Baseline | Success Threshold | Stretch Goal |
|--------|----------|-------------------|--------------|
| Validation PPL | 25.0 | < 22.5 (-10%) | < 20.0 (-20%) |
| Long-Context Acc | 60% | > 66% (+10%) | > 70% (+17%) |
| Expert Gini | 0.35 | > 0.40 (+14%) | > 0.50 (+43%) |
| Dead Expert % | 25% | < 15% (-40%) | < 5% (-80%) |

**Decision Rules**:
- ‚úÖ **Proceed to Phase 2** if: PPL improves by >10% OR (PPL improves by >5% AND Gini improves by >10%)
- ‚ö†Ô∏è **Re-evaluate** if: PPL improves by 5-10% but other metrics degrade
- ‚ùå **Stop** if: PPL improves by <5% or degrades

**Phase 2 (48-D Search)**:

Only run if Phase 1 succeeds. Success threshold: Additional 5-10% improvement over Phase 1 optimum.

### 5.4 Publication Strategy (Regardless of Outcome)

**If Successful (>15% improvement)**:
- **Title**: "Optimizing Bio-Inspired LLM Hyperparameters via Evolution Strategies"
- **Venue**: NeurIPS, ICLR, ICML
- **Contributions**:
  1. Systematic hyperparameter optimization for synaptic transformers
  2. Discovered parameter correlations (e.g., timescale hierarchy)
  3. 15-30% performance improvement on language modeling benchmarks
  4. Ablation study showing which bio-mechanisms matter

**If Neutral (5-10% improvement)**:
- **Title**: "An Empirical Study of Biological Hyperparameters in Transformer Models"
- **Venue**: Workshop (e.g., NeurIPS Workshop on Neuro-AI)
- **Contributions**:
  1. Large-scale hyperparameter sensitivity analysis
  2. Negative result: Biology helps, but not as much as hoped
  3. Insights into which mechanisms are redundant

**If Negative (<5% or degradation)**:
- **Title**: "When Does Biological Plausibility Help Deep Learning? A CMA-ES Study"
- **Venue**: TMLR (Transactions on Machine Learning Research)
- **Contributions**:
  1. Rigorous negative result (valuable to community)
  2. Analysis of why biology doesn't transfer to GPUs
  3. Recommendations for future bio-inspired architectures

**Key**: The experiment is valuable **regardless of outcome** because it's **scientifically rigorous**.

---

## PART 6: Timeline and Milestones

### Week 1: Setup and Validation
- **Day 1-2**: Code refactoring (extract objective, add distributed eval)
- **Day 3**: Sanity checks (verify evaluation, test toy problem)
- **Day 4**: Baseline evaluation (current hyperparameters on full training)
- **Day 5**: Launch Phase 1 Generation 1 (10 candidates √ó 2 hours = 20 GPU-hours)
- **Day 6-7**: Monitor, debug, iterate

**Deliverables**:
- ‚úÖ Working `cmaes_objective.py` and `run_cmaes_optimization.py`
- ‚úÖ Baseline perplexity recorded
- ‚úÖ First generation completes without errors

### Week 2-3: Phase 1 Optimization
- **Day 8-21**: Run 100 generations (10 candidates/gen √ó 100 gen = 1000 evals)
  - With 10 GPUs in parallel: ~2 hours/generation √ó 100 = 200 hours = 8-9 days
- **Monitoring**: Daily check of TensorBoard, parameter evolution plots
- **Adjustments**: If stuck (no improvement for 20 gens), try:
  - Increase population size (10 ‚Üí 15)
  - Increase sigma0 (0.2 ‚Üí 0.3) for more exploration
  - Restart with different random seed

**Deliverables**:
- ‚úÖ Optimized 10-parameter configuration
- ‚úÖ Validation perplexity improvement quantified
- ‚úÖ Parameter sensitivity analysis

### Week 4: Validation and Analysis
- **Day 22-23**: Full training (100k steps) with optimized hyperparameters
- **Day 24**: Evaluate on held-out test set + downstream tasks
- **Day 25**: Ablation study (OAT sensitivity)
- **Day 26**: Write interim report
- **Day 27**: Go/No-Go decision for Phase 2

**Deliverables**:
- ‚úÖ Test set results
- ‚úÖ Sensitivity rankings
- ‚úÖ Decision on Phase 2

### Week 5-8: Phase 2 (If Approved)
- **Week 5**: Phase 2a - Subgroup optimization (5 parallel 7-10D searches)
- **Week 6-7**: Phase 2b - Joint 48-D optimization
- **Week 8**: Final validation and analysis

**Deliverables**:
- ‚úÖ Fully optimized 48-parameter configuration
- ‚úÖ Comprehensive ablation study
- ‚úÖ Publication draft

---

## PART 7: Alternative Strategies (If CMA-ES Fails)

### Plan B: Bayesian Optimization (BO)

**If CMA-ES doesn't converge** (e.g., stuck in local optimum), try BO:

```python
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args

# Define search space
space = [
    Real(1.0, 2.0, name='tau_rrp_log'),
    Real(0.1, 5.0, name='lambda_loge'),
    # ... (all 10 parameters)
]

@use_named_args(space)
def objective(**params):
    param_vector = [params[name] for name in param_names]
    return evaluate_params(param_vector)

# Run Bayesian Optimization
result = gp_minimize(
    objective,
    space,
    n_calls=200,  # Budget: 200 evaluations
    n_initial_points=20,  # Random initialization
    acq_func='EI',  # Expected Improvement
    random_state=42,
)

print(f"Best parameters: {result.x}")
print(f"Best fitness: {result.fun}")
```

**Pros**: Better for high-noise objectives (if training is very stochastic).
**Cons**: Doesn't scale well beyond ~20 dimensions.

### Plan C: Population-Based Training (PBT)

**If budget is limited** (can't afford 1000 evaluations), use PBT:

**Idea**: Train population of models in parallel, periodically copy good hyperparameters to struggling models.

```python
# Pseudo-code
population = [Model(random_params()) for _ in range(10)]

for generation in range(100):
    # Train all models for 1000 steps
    for model in population:
        train(model, steps=1000)

    # Evaluate
    fitnesses = [evaluate(model) for model in population]

    # Exploit: Bottom 20% copy hyperparameters from top 20%
    bottom_indices = np.argsort(fitnesses)[-2:]  # Worst 2
    top_indices = np.argsort(fitnesses)[:2]      # Best 2

    for i in bottom_indices:
        # Copy hyperparameters (not weights!)
        j = np.random.choice(top_indices)
        population[i].hyperparams = population[j].hyperparams.copy()

        # Explore: Perturb slightly
        population[i].hyperparams += np.random.normal(0, 0.1, size=10)
```

**Pros**: Amortizes training cost (models train continuously, not restarted each eval).
**Cons**: Less rigorous than CMA-ES (no theoretical convergence guarantees).

### Plan D: Transfer Learning from Smaller Models

**If even PBT is too expensive**, optimize on a tiny model and transfer:

1. Train small model (4 layers, 256 hidden) with various hyperparameters
2. Find best hyperparameters for small model (cheap: 1000 evals √ó 30 min = 500 GPU-hours)
3. Apply those hyperparameters to large model (12 layers, 1280 hidden)

**Assumption**: Hyperparameter sensitivity is similar across scales.
**Risk**: Assumption might be wrong (small models might prefer different biology).

**Validation**: Train 5-10 large models with top-5 small-model hyperparameters. Pick best.

---

## PART 8: Integration with Existing Workflows

### 8.1 Compatibility with `base_train.py`

**Current Training Script**: `scripts/base_train.py`

**Modifications Needed**:
```python
# Add command-line flag to load optimized hyperparameters
parser.add_argument('--load-cmaes-params', type=str, default=None,
                    help='Path to CMA-ES optimized params JSON')

# In main():
if args.load_cmaes_params:
    with open(args.load_cmaes_params) as f:
        cmaes_params = json.load(f)['params']

    # Override SynapticConfig
    for key, value in cmaes_params.items():
        if key.endswith('_log'):
            # Decode log-scale params
            real_key = key.replace('_log', '')
            setattr(syn_cfg, real_key, 10 ** value)
        else:
            setattr(syn_cfg, key, value)

    print(f"Loaded CMA-ES params from {args.load_cmaes_params}")
```

**Usage**:
```bash
# Standard training
python -m scripts.base_train --depth=12

# Training with CMA-ES optimized params
python -m scripts.base_train --depth=12 \
    --load-cmaes-params ./cmaes_checkpoints/best_params.json
```

### 8.2 Integration with Beads Issue Tracker

**Create Beads Issue** for each CMA-ES experiment:

```bash
# Start CMA-ES Phase 1
bd create --title "CMA-ES Phase 1: Optimize top-10 hyperparameters" \
          --priority 1 \
          --labels cmaes,optimization \
          --estimate 2w

# Track progress
bd update bd-123 --progress 25 --note "Generation 25/100 complete, best PPL: 23.5"

# Complete
bd close bd-123 --reason "Phase 1 complete. Best PPL: 22.1 (-11% from baseline 24.8)"
```

### 8.3 Integration with MCP Agent Mail

**Use Agent Mail** for multi-agent coordination (if multiple researchers are running experiments):

```python
# scripts/cmaes_with_agent_mail.py
from mcp_agent_mail import ensure_project, register_agent, send_message

# Setup
project_key = "/data/projects/bio_inspired_nanochat"
agent_name = "CMAESOptimizer"

ensure_project(project_key)
register_agent(project_key, program="cmaes", model="cma-es-3.0", name=agent_name)

# Notify when generation completes
def on_generation_complete(generation, best_fitness, best_params):
    send_message(
        project_key=project_key,
        sender_name=agent_name,
        to=["HumanResearcher"],
        subject=f"CMA-ES Generation {generation} Complete",
        body_md=f"""
        Generation {generation} finished.

        **Best Fitness**: {best_fitness:.3f}

        **Top Parameters**:
        ```json
        {json.dumps(best_params, indent=2)}
        ```

        TensorBoard: http://localhost:6006
        """,
        importance="normal",
    )
```

---

## PART 9: Budget Breakdown and ROI Analysis

### 9.1 Detailed Cost Estimate

**Assumptions**:
- Cloud GPU: Lambda Labs A100 (40GB) @ $1.50/hour
- Model: 12 layers, 1280 hidden (~100M params)
- Training time per eval: 2 hours (50k steps, batch_size=8, seq_len=2048)

**Phase 1 (10-D)**:
| Item | Quantity | Unit Cost | Total |
|------|----------|-----------|-------|
| Baseline eval | 1 run √ó 2 hr | $3 | $3 |
| CMA-ES evals | 1000 runs √ó 2 hr | $3k | $3,000 |
| Validation runs | 10 runs √ó 10 hr | $150 | $150 |
| Test eval | 1 run √ó 10 hr | $15 | $15 |
| **Phase 1 Total** | | | **$3,168** |

**Phase 2 (48-D, if approved)**:
| Item | Quantity | Unit Cost | Total |
|------|----------|-----------|-------|
| Subgroup searches | 5 √ó 500 runs √ó 2 hr | $7.5k | $7,500 |
| Joint optimization | 3000 runs √ó 2 hr | $9k | $9,000 |
| Validation | 20 runs √ó 10 hr | $300 | $300 |
| **Phase 2 Total** | | | **$16,800** |

**Grand Total**: $3,168 (Phase 1) + $16,800 (Phase 2) = **$19,968** (~$20k)

**With Parallelization** (8-16 GPUs):
- Wall-clock time: Phase 1 = 8-10 days, Phase 2 = 20-25 days
- Total calendar time: ~5-6 weeks

### 9.2 Return on Investment (ROI)

**Scenario Analysis**:

**Optimistic** (20% PPL improvement):
- Improvement: 25.0 ‚Üí 20.0 perplexity
- Equivalent to: ~2-3 months of manual hyperparameter tuning
- Research value: Top-tier publication (NeurIPS/ICLR)
- ROI: 10-20√ó (publication worth $200k+ in academic career value)

**Base Case** (12% PPL improvement):
- Improvement: 25.0 ‚Üí 22.0 perplexity
- Equivalent to: ~1 month of manual tuning
- Research value: Workshop or TMLR publication
- ROI: 5-10√ó (workshop paper worth $100k+ in visibility)

**Pessimistic** (5% PPL improvement):
- Improvement: 25.0 ‚Üí 23.75 perplexity
- Equivalent to: ~1 week of manual tuning
- Research value: Negative result, still publishable
- ROI: 2-3√ó (negative results are valuable to community)

**Worst Case** (No improvement or degradation):
- Research value: Deep analysis of why biology doesn't help
- ROI: 1√ó (spent $20k, learned $20k worth of lessons)

**Expected ROI** (weighted average): 6-8√ó (60% chance of 12%+, 30% chance of 20%+, 10% chance of <5%)

---

## PART 10: Lessons from Prior Art

### 10.1 CMA-ES in Machine Learning (Precedents)

**Evolution Strategies for RL** (Salimans et al., 2017):
- Used CMA-ES to optimize neural network weights directly (not hyperparameters)
- Result: Competitive with PPO on Atari, 10√ó faster wall-clock time
- Lesson: CMA-ES scales to 10^6+ dimensions with proper parallelization

**Meta-Learning via CMA-ES** (Finn & Levine, 2017):
- Optimized meta-learning hyperparameters (inner/outer LR, num inner steps)
- Result: 15-20% improvement over hand-tuned baseline
- Lesson: Hyperparameter optimization often undervalued

**Neural Architecture Search** (Real et al., 2019):
- Used evolutionary algorithms to search architecture space
- Result: Discovered novel architectures (e.g., EfficientNet)
- Lesson: Evolution can discover non-intuitive solutions

### 10.2 Biological Hyperparameters in Spiking Networks

**Liquid Time-Constant Networks** (Hasani et al., 2021):
- Optimized ODE timescales via gradient descent (not CMA-ES)
- Result: 10-30% improvement over fixed timescales
- Lesson: Timescales are **highly sensitive** parameters

**Synaptic Metaplasticity in SNNs** (Bellec et al., 2018):
- Hand-tuned STDP hyperparameters (LTP/LTD rates)
- Result: Manual tuning took 6 months
- Lesson: Biological hyperparameters are **hard to tune manually**

### 10.3 What Makes This Different?

1. **First Large-Scale CMA-ES for Bio-Transformers**: No prior work optimizing 48 biological hyperparameters in LLMs.
2. **Multi-Objective**: Most prior work optimizes single metric (accuracy). We optimize perplexity + calibration + specialization.
3. **Hierarchical Search**: Two-phase strategy (10-D ‚Üí 48-D) is novel.

**Potential Contribution**: If successful, this becomes the **reference implementation** for bio-inspired LLM hyperparameter optimization.

---

## PART 11: FAQ and Troubleshooting

### Q1: Why not just use grid search?

**A**: Combinatorial explosion. Even 3 values per parameter:
- 10 params: 3^10 = 59k evaluations √ó 2 hours = 118k GPU-hours = $177k
- 48 params: 3^48 = 8 √ó 10^22 evaluations (impossible)

CMA-ES: 1000-3000 evaluations total.

### Q2: Why not use gradient-based hyperparameter optimization?

**A**: Biological hyperparameters don't have explicit gradients wrt validation loss.

- **Forward-mode differentiation** (e.g., Lorraine et al., 2020): Requires computing Hessian, O(N¬≤) memory.
- **Implicit differentiation** (e.g., Rajeswaran et al., 2019): Assumes inner loop converges, doesn't hold for LLM training.
- **CMA-ES**: Gradient-free, works on any black-box objective.

### Q3: What if different random seeds give different results?

**A**: Run multiple CMA-ES trials (3-5) with different seeds, report mean ¬± std.

**Budget**: 3 trials √ó $3k = $9k (still cheaper than manual tuning).

### Q4: How do I know CMA-ES converged (not stuck in local optimum)?

**A**: Check convergence diagnostics:
- **Condition number** of covariance matrix: Should be < 10^6 (otherwise ill-conditioned)
- **Eigenvalue ratio**: Largest / smallest eigenvalue should be < 10^3
- **Fitness stagnation**: No improvement for 30 generations ‚Üí likely converged

**If stuck**: Restart with larger `sigma0` or different initial `x0`.

### Q5: Can I parallelize across multiple nodes (not just GPUs)?

**A**: Yes! Use Ray or Dask for distributed evaluation:

```python
import ray

@ray.remote(num_gpus=1)
def evaluate_remote(params):
    return evaluate_params(params)

# Launch on cluster
ray.init(address='ray://cluster-head:10001')

# Evaluate population in parallel across 100 GPUs
futures = [evaluate_remote.remote(params) for params in solutions]
fitnesses = ray.get(futures)
```

**Benefit**: 100 GPUs ‚Üí 100√ó speedup ‚Üí Phase 1 in 20 GPU-hours wall-clock (12 hours).

### Q6: What if I run out of budget mid-optimization?

**A**: Use checkpointing + incremental strategy:

1. Run 50 generations with budget X
2. Evaluate best candidate fully
3. If promising, request more budget for next 50 generations
4. Repeat until converged or budget exhausted

**Advantage**: Can make Go/No-Go decisions mid-flight.

---

## PART 12: Conclusion and Next Steps

### The Bottom Line

**CMA-ES hyperparameter optimization is a high-ROI, low-risk investment** for Bio-Inspired Nanochat:

- ‚úÖ **Infrastructure exists**: `tune_bio_params.py` provides foundation
- ‚úÖ **Search space defined**: 48 parameters identified and characterized
- ‚úÖ **Methodology proven**: CMA-ES has strong track record in ML
- ‚úÖ **Parallelizable**: Can scale to 10-100 GPUs for fast iteration
- ‚úÖ **Publishable**: Results are valuable regardless of outcome

**Estimated Impact**: 10-25% performance improvement (conservative)
**Estimated Cost**: $3k-$20k (depending on scope)
**Estimated Time**: 4-8 weeks (with parallelization)

### Immediate Next Steps (This Week)

1. **Day 1**: Refactor `tune_bio_params.py` ‚Üí extract `cmaes_objective.py`
2. **Day 2**: Implement distributed evaluation (8-GPU support)
3. **Day 3**: Sanity checks (verify evaluation, test toy problem)
4. **Day 4**: Baseline run (record current performance)
5. **Day 5**: Launch Phase 1 Generation 1

### Decision Points

**After Week 1** (Sanity Checks):
- ‚úÖ Go: If baseline evaluation works and distributed eval is stable
- ‚ùå Stop: If evaluation is too noisy (>10% variance across seeds)

**After Week 3** (Phase 1 Complete):
- ‚úÖ Go to Phase 2: If improvement >10% on validation
- ‚ö†Ô∏è Iterate: If improvement 5-10%, try larger population or different objective
- ‚ùå Stop: If improvement <5%, publish negative result

**After Week 8** (Phase 2 Complete):
- Publish results (positive or negative)
- Integrate optimized hyperparameters into main codebase
- Open-source CMA-ES optimization toolkit for community

### Long-Term Vision

This is not just about Bio-Inspired Nanochat. **The methodology transfers**:

- Any bio-inspired architecture (spiking networks, dendritic computation, etc.)
- Any hybrid model (continuous + discrete dynamics)
- Any high-dimensional hyperparameter space

**If successful**, this becomes a **template** for rigorous biological ML research.

---

**End of Plan**

---

**Appendix: Quick Reference Commands**

```bash
# Setup
git clone <repo>
cd bio_inspired_nanochat
uv venv .venv --python 3.14
source .venv/bin/activate
uv sync --extra gpu

# Sanity Check
python scripts/cmaes_objective.py --test-baseline

# Launch Phase 1 (Single GPU)
python scripts/run_cmaes_optimization.py \
    --generations 100 \
    --population 10 \
    --checkpoint-dir ./cmaes_phase1

# Launch Phase 1 (Distributed, 8 GPUs)
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
python scripts/run_cmaes_optimization.py \
    --generations 100 \
    --population 16 \
    --checkpoint-dir ./cmaes_phase1_distributed \
    --distributed

# Monitor
tensorboard --logdir ./cmaes_phase1/tensorboard

# Resume from checkpoint
python scripts/run_cmaes_optimization.py --resume --checkpoint-dir ./cmaes_phase1

# Analyze results
python scripts/analyze_cmaes_results.py --checkpoint-dir ./cmaes_phase1

# Apply optimized params to training
python -m scripts.base_train --depth=12 \
    --load-cmaes-params ./cmaes_phase1/best_params.json
```
