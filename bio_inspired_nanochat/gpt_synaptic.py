# nanochat/gpt_synaptic.py
# pylint: disable=too-many-instance-attributes
from __future__ import annotations

# GPT with Synaptic Attention/MLP and optional Synaptic MoE + structural hooks

from dataclasses import dataclass, field
from typing import Optional

from bio_inspired_nanochat.torch_imports import torch, nn, F, Tensor

from bio_inspired_nanochat.common import ca_init_weight_

from .synaptic import (
    PostsynapticHebb,
    SynapticCausalSelfAttention,
    SynapticMLP,
    SynapticConfig,
    SynapticMoE,
    SynapticLinear,
    SynapticPresyn,
    StructuralPlasticity,
)


# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------


@dataclass
class GPTSynapticConfig:
    sequence_len: int = 2048
    vocab_size: int = 65536
    n_layer: int = 20
    n_head: int = 10
    n_kv_head: int = 10
    n_embd: int = 1280
    rope_base: float = 10000.0
    synapses: bool = True
    syn_cfg: SynapticConfig = field(default_factory=SynapticConfig)
    dropout: float = 0.0
    # MoE & structural options
    use_moe: bool = False
    num_experts: int = 8
    moe_top_k: int = 2
    moe_hidden_mult: int = 4
    moe_balance_loss: float = 0.01
    structural_every: int = 0  # 0 → off; >0 → run hooks every N blocks
    # Weight initialization
    init_type: str = "baseline"  # "baseline" | "ca_rule30" | "ca_rule116"
    init_seed: int = 42


# -----------------------------------------------------------------------------
# Blocks
# -----------------------------------------------------------------------------


class MLP(nn.Module):
    def __init__(self, n_embd: int, syn_cfg: SynapticConfig, dropout: float = 0.0):
        super().__init__()
        self.mlp = SynapticMLP(n_embd, syn_cfg, dropout)

    def forward(self, x):
        return self.mlp(x)


class CausalSelfAttention(nn.Module):
    def __init__(
        self,
        layer_idx: int,
        n_embd: int,
        n_head: int,
        n_kv_head: int,
        rope_cos: Tensor,
        rope_sin: Tensor,
        syn_cfg: SynapticConfig,
        attn_drop=0.0,
        resid_drop=0.0,
    ):
        super().__init__()
        self.attn = SynapticCausalSelfAttention(
            n_embd,
            n_head,
            n_kv_head,
            rope_cos,
            rope_sin,
            syn_cfg,
            layer_idx,
            attn_drop,
            resid_drop,
        )

    def forward(self, x, kv_cache=None, presyn_state=None, train_mode=True):
        y, st = self.attn(x, kv_cache, presyn_state, train_mode)
        return y, st


class Block(nn.Module):
    def __init__(
        self,
        layer_idx: int,
        n_embd: int,
        n_head: int,
        n_kv_head: int,
        rope_cos: Tensor,
        rope_sin: Tensor,
        syn_cfg: SynapticConfig,
        dropout: float = 0.0,
        use_moe: bool = False,
        num_experts: int = 8,
        top_k: int = 2,
        hidden_mult: int = 4,
        balance_loss: float = 0.01,
    ):
        super().__init__()
        self.norm1 = nn.LayerNorm(n_embd)
        self.attn = CausalSelfAttention(
            layer_idx,
            n_embd,
            n_head,
            n_kv_head,
            rope_cos,
            rope_sin,
            syn_cfg,
            attn_drop=dropout,
            resid_drop=dropout,
        )
        self.norm2 = nn.LayerNorm(n_embd)
        self.use_moe: bool = use_moe
        self.balance_loss: float = float(balance_loss)
        self.last_aux_loss: Tensor = torch.tensor(0.0)
        self.mlp = (
            SynapticMoE(n_embd, num_experts, top_k, hidden_mult, syn_cfg, dropout)
            if use_moe
            else MLP(n_embd, syn_cfg, dropout)
        )

    def forward(self, x, kv_cache=None, presyn_state=None, train_mode=True):
        a, st = self.attn(self.norm1(x), kv_cache, presyn_state, train_mode)
        x = x + a
        if self.use_moe:
            y, aux = self.mlp(self.norm2(x))
            self.last_aux_loss = self.balance_loss * aux
        else:
            y = self.mlp(self.norm2(x))
            self.last_aux_loss = torch.tensor(0.0, device=x.device)
        x = x + y
        return x, st


# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------


class GPTSynaptic(nn.Module):
    def __init__(self, config: GPTSynapticConfig):
        super().__init__()
        c = config
        self.config: GPTSynapticConfig = c
        self.wte: nn.Embedding = nn.Embedding(c.vocab_size, c.n_embd)
        self.h: nn.ModuleList[Block] = nn.ModuleList()
        self.transformer = nn.ModuleDict(dict(wte=self.wte, h=self.h))
        self.lm_head = nn.Linear(c.n_embd, c.vocab_size, bias=False)
        self.drop = nn.Dropout(c.dropout)
        nn.init.trunc_normal_(self.lm_head.weight, std=0.02)
        T = c.sequence_len
        hd = c.n_embd // c.n_head
        base = c.rope_base
        inv_freq: Tensor = 1.0 / (
            base ** (torch.arange(0, hd // 2, dtype=torch.float32) / (hd // 2))
        )
        t: Tensor = torch.arange(0, T * 8, dtype=torch.float32)
        freqs: Tensor = torch.outer(t, inv_freq)
        self.cos: Tensor
        self.register_buffer(
            "cos", torch.cos(freqs).unsqueeze(0).to(torch.bfloat16), persistent=False
        )
        self.sin: Tensor
        self.register_buffer(
            "sin", torch.sin(freqs).unsqueeze(0).to(torch.bfloat16), persistent=False
        )
        for _ in range(c.n_layer):
            layer_idx = len(self.h)
            self.h.append(
                Block(
                    layer_idx,
                    c.n_embd,
                    c.n_head,
                    c.n_kv_head,
                    self.cos,
                    self.sin,
                    c.syn_cfg,
                    dropout=c.dropout,
                    use_moe=c.use_moe,
                    num_experts=c.num_experts,
                    top_k=c.moe_top_k,
                    hidden_mult=c.moe_hidden_mult,
                    balance_loss=c.moe_balance_loss,
                )
            )

    def estimate_flops(self):
        L = self.config.n_layer
        N = self.config.n_embd
        H = self.config.n_head
        return 6 * L * N * N + 4 * L * N * H * 128

    def forward(
        self,
        idx: Tensor,
        targets: Optional[Tensor] = None,
        kv_cache=None,
        train_mode=True,
    ):
        B, T = idx.size()
        assert T <= self.config.sequence_len
        tok = self.wte(idx)
        x = self.drop(tok)
        
        # Initialize per-layer presynaptic state from kv_cache if available
        presyn_states = None
        if kv_cache is not None and hasattr(kv_cache, "presyn_state"):
            presyn_states = kv_cache.presyn_state

        def _clone_presyn_state(state):
            if state is None:
                return None
            cloned = {}
            for key, value in state.items():
                if isinstance(value, list):
                    new_queue = []
                    for item in value:
                        if torch.is_tensor(item):
                            new_queue.append(item.clone())
                        else:
                            new_queue.append(item)
                    cloned[key] = new_queue
                elif torch.is_tensor(value):
                    cloned[key] = value.clone()
                else:
                    cloned[key] = value
            return cloned

        if isinstance(presyn_states, list):
            if len(presyn_states) < len(self.h):
                presyn_states = presyn_states + [None] * (len(self.h) - len(presyn_states))
            elif len(presyn_states) > len(self.h):
                presyn_states = presyn_states[: len(self.h)]
        elif isinstance(presyn_states, dict):
            # Backward-compat: expand a single dict into per-layer copies
            presyn_states = [_clone_presyn_state(presyn_states) for _ in range(len(self.h))]
        else:
            presyn_states = [None] * len(self.h)

        for li, block in enumerate(self.h):
            layer_state = presyn_states[li]
            x, layer_state = block(x, kv_cache, layer_state, train_mode)
            presyn_states[li] = layer_state
            if self.config.structural_every and targets is not None:
                if (li + 1) % self.config.structural_every == 0 and hasattr(
                    block.mlp, "experts"
                ):
                    # Hook point for split/merge (kept as a callable point on purpose)
                    pass
        
        # Save presyn_state back to kv_cache
        if kv_cache is not None:
            # We attach it dynamically if it doesn't exist in __init__ yet (though we should add it there too)
            kv_cache.presyn_state = presyn_states

        logits = self.lm_head(x.to(dtype=self.lm_head.weight.dtype))
        if targets is None:
            return logits, None
        aux = sum(
            (
                getattr(b, "last_aux_loss", torch.tensor(0.0, device=logits.device))
                for b in self.h
            )
        )
        logits_flat = logits.reshape(-1, logits.size(-1))
        targets_flat = targets.reshape(-1)
        ce = F.cross_entropy(
            logits_flat,
            targets_flat,
            ignore_index=-1,
            reduction="mean",
        )
        loss = ce + aux
        return logits, loss

    def setup_optimizers(
        self,
        unembedding_lr=0.004,
        embedding_lr=0.2,
        matrix_lr=0.02,
        weight_decay=0.0,
        lr=None,
        wd=None,
    ):
        # Support both old GPT-style signature and new simple signature
        # If GPT-style args are provided (defaults), use GPT-style optimizer setup
        # Otherwise use simple single optimizer
        if lr is not None and wd is not None:
            # New simple signature (explicit lr/wd provided)
            no_decay, set_decay = set(), set()
            for n, p in self.named_parameters():
                if p.ndim < 2 or "lm_head" in n or "wte" in n:
                    no_decay.add(n)
                else:
                    set_decay.add(n)
            optim_groups = [
                {
                    "params": [p for n, p in self.named_parameters() if n in set_decay],
                    "weight_decay": wd,
                },
                {
                    "params": [p for n, p in self.named_parameters() if n in no_decay],
                    "weight_decay": 0.0,
                },
            ]
            opt = torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8)
            return [opt]  # Return as list for compatibility
        else:
            # GPT-style signature (for compatibility with existing training scripts)
            from bio_inspired_nanochat.common import get_dist_info
            from bio_inspired_nanochat.muon import Muon, DistMuon
            from bio_inspired_nanochat.adamw import DistAdamW
            from functools import partial

            model_dim = self.config.n_embd
            ddp, rank, local_rank, world_size = get_dist_info()
            
            # Separate matrix params (2D) for Muon from other params (1D/0D) for AdamW
            matrix_params = []
            other_params = []
            
            # Collect params from transformer blocks
            for p in self.h.parameters():
                if p.ndim >= 2:
                    matrix_params.append(p)
                else:
                    other_params.append(p)
            
            embedding_params = list(self.wte.parameters())
            lm_head_params = list(self.lm_head.parameters())
            
            dmodel_lr_scale = (model_dim / 768) ** -0.5
            if rank == 0:
                print(
                    f"Scaling the LR for the AdamW parameters ∝1/√({model_dim}/768) = {dmodel_lr_scale:.6f}"
                )
            
            # AdamW gets embedding, lm_head, and all 1D/0D params from blocks (biases, layernorms)
            adam_groups = [
                dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),
                dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),
                dict(params=other_params, lr=embedding_lr * dmodel_lr_scale), # Use embedding LR scale for other params? Or maybe just matrix_lr? Usually AdamW params get higher LR.
            ]
            adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)
            adam_params = embedding_params + lm_head_params + other_params
            use_fused = (not ddp) and any(p.is_cuda for p in adam_params)
            AdamWFactory = DistAdamW if ddp else partial(torch.optim.AdamW, fused=use_fused)
            adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)
            
            muon_kwargs = dict(lr=matrix_lr, momentum=0.95)
            MuonFactory = DistMuon if ddp else Muon
            muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)
            optimizers = [adamw_optimizer, muon_optimizer]
            for opt in optimizers:
                for group in opt.param_groups:
                    group["initial_lr"] = group["lr"]
            return optimizers

    @torch.no_grad()
    def init_weights(self):
        """Initialize weights after `to_empty` (meta-device safe)."""
        init_type = str(getattr(self.config, "init_type", "baseline"))
        init_seed = int(getattr(self.config, "init_seed", 42))

        # 1) Baseline initialization (mirrors what we'd want on a non-meta constructor path).
        for module_name, module in self.named_modules():
            if module is self:
                continue

            if isinstance(module, nn.Linear):
                if module_name == "lm_head":
                    nn.init.trunc_normal_(module.weight, std=0.02)
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                else:
                    module.reset_parameters()
                continue

            if isinstance(module, nn.Embedding):
                module.reset_parameters()
                continue

            if isinstance(module, SynapticLinear):
                nn.init.trunc_normal_(module.w_slow, std=0.02)
                if module.w_fast is not None:
                    nn.init.trunc_normal_(module.w_fast, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
                if module.u_buf is not None:
                    module.u_buf.zero_()
                if module.v_buf is not None:
                    module.v_buf.zero_()
                continue

            if isinstance(module, PostsynapticHebb):
                module.fast.zero_()
                module.slow.zero_()
                nn.init.normal_(module.U, std=0.02)
                nn.init.normal_(module.V, std=0.02)
                module.camkii.zero_()
                module.pp1.fill_(0.5)
                module.bdnf.zero_()
                if hasattr(module, "bdnf_hebb_accum"):
                    module.bdnf_hebb_accum.zero_()
                if hasattr(module, "_last_hebb_delta_mag"):
                    module._last_hebb_delta_mag.zero_()
                continue

            if isinstance(module, SynapticPresyn):
                module.ema_e.fill_(1.0)
                continue

            if isinstance(module, SynapticMLP):
                module.C0.fill_(0.5)
                module.E0.fill_(0.8)
                continue

            if isinstance(module, SynapticMoE):
                module.fatigue.zero_()
                module.energy.fill_(1.0)
                emb = torch.randn(
                    module.num_experts,
                    module.cfg.router_embed_dim,
                    device=module.router_embeddings.device,
                    dtype=module.router_embeddings.dtype,
                )
                emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)
                module.router_embeddings.copy_(emb)
                nn.init.normal_(module.Xi, std=0.1)
                continue

            if isinstance(module, StructuralPlasticity):
                module.age.zero_()
                module.util.zero_()
                continue

            if hasattr(module, "reset_parameters"):
                module.reset_parameters()

        # 2) Optional CA override for selected matrices (keeps embeddings/head baseline).
        if init_type.startswith("ca_") or init_type.startswith("ca"):
            rule = 116 if "116" in init_type else 30
            max_ca_fan_out = 8192

            for module_name, module in self.named_modules():
                if module is self:
                    continue

                if isinstance(module, nn.Linear):
                    if module_name == "lm_head":
                        continue
                    fan_out = int(module.weight.size(0))
                    if fan_out > max_ca_fan_out:
                        continue
                    ca_init_weight_(
                        module.weight,
                        rule=rule,
                        seed=init_seed,
                        salt=f"{module_name}.weight",
                        layout="out_in",
                    )
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                    continue

                if isinstance(module, SynapticLinear):
                    fan_out = int(module.w_slow.size(1))
                    if fan_out > max_ca_fan_out:
                        continue
                    ca_init_weight_(
                        module.w_slow,
                        rule=rule,
                        seed=init_seed,
                        salt=f"{module_name}.w_slow",
                        layout="in_out",
                    )
                    if module.w_fast is not None:
                        ca_init_weight_(
                            module.w_fast,
                            rule=rule,
                            seed=init_seed,
                            salt=f"{module_name}.w_fast",
                            layout="in_out",
                        )
                    continue

        # 3) Rebuild RoPE buffers (meta-device safe) and re-link attention modules.
        T = int(self.config.sequence_len)
        hd = int(self.config.n_embd // self.config.n_head)
        base = float(self.config.rope_base)
        device = self.wte.weight.device

        inv_freq: Tensor = 1.0 / (
            base ** (torch.arange(0, hd // 2, dtype=torch.float32, device=device) / (hd // 2))
        )
        t: Tensor = torch.arange(0, T * 8, dtype=torch.float32, device=device)
        freqs: Tensor = torch.outer(t, inv_freq)
        cos = torch.cos(freqs).unsqueeze(0).to(torch.bfloat16)
        sin = torch.sin(freqs).unsqueeze(0).to(torch.bfloat16)
        self.cos.copy_(cos)
        self.sin.copy_(sin)

        for _, module in self.named_modules():
            if isinstance(module, SynapticCausalSelfAttention):
                module.cos = self.cos
                module.sin = self.sin

    def get_device(self):
        return self.wte.weight.device
