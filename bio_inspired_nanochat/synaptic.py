# nanochat/synaptic.py
# Comprehensive synaptic modules for nanochat:
# - Presynaptic biophysics → attention logit augmentation
# - Postsynaptic dual-timescale linear with low-rank eligibility
# - Synaptic Self-Attention (RoPE, MQA-compatible)
# - Synaptic MLP
# - Synaptic MoE with router embeddings, contrastive updates & structural hooks
# - Structural plasticity utilities
#
# Design highlights (mapped from the JAX reference you provided):
#   • Synaptotagmin-1/7 mixed Ca2+ sensor, complexin clamp
#   • Munc13/18 priming, clathrin/dynamin endocytosis (delay queue)
#   • V-ATPase/VDAC energy coupling and per-edge cost model
#   • EMA normalization of quantal gain; optional stochastic release
#   • PSD-like low-rank eligibility U/V with CaMKII/PP1 gating (fast/slow)
#   • Septin-like distance barrier in attention logits
#   • Router embeddings + contrastive update; MoE top-k dispatch with fatigue
#
# This file is intentionally verbose and highly instrumented for clarity.

import math
from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, cast, Any

from bio_inspired_nanochat.torch_imports import torch, nn, F, Tensor
from decouple import Config as DecoupleConfig, RepositoryEnv

# Initialize decouple config
try:
    decouple_config = DecoupleConfig(RepositoryEnv(".env"))
except Exception:
    from decouple import Config, AutoConfig
    decouple_config = Config(RepositoryEnv(".env")) if False else AutoConfig()


# -----------------------------------------------------------------------------
# Utility functions
# -----------------------------------------------------------------------------


def _rmsnorm(x: Tensor, eps=1e-6) -> Tensor:
    return torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + eps) * x


def _tri(T: int, device: torch.device, dtype: torch.dtype) -> Tensor:
    return torch.tril(torch.ones(T, T, device=device, dtype=dtype)).view(1, 1, T, T)


def _softplus(x: Tensor, beta=1.0) -> Tensor:
    return (1.0 / beta) * F.softplus(beta * x)


def _cosine(u: Tensor, v: Tensor, eps=1e-8) -> Tensor:
    """Cosine similarity with safe normalization."""
    u = u / (u.norm(dim=-1, keepdim=True) + eps)
    v = v / (v.norm(dim=-1, keepdim=True) + eps)
    return (u * v).sum(dim=-1)


# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------


@dataclass
class SynapticConfig:
    enabled: bool = True
    # General
    rank_eligibility: int = 8
    attn_topk: int = 32
    stochastic_train_frac: float = 0.12
    
    # Presynaptic Biophysics
    tau_c: float = 0.85
    alpha_c: float = 0.55
    syt1_slope: float = 8.0
    syt7_slope: float = 3.0
    cpx_thresh: float = 0.55
    doc2_gain: float = 0.08
    prime_rate: float = 0.075
    unprime_per_release: float = 0.05
    nsf_recover: float = 0.08
    rec_rate: float = 0.06
    endo_delay: int = 3
    amp_load: float = 0.02
    amp_leak: float = 0.006
    
    # Initial States
    init_rrp: float = 6.0
    init_reserve: float = 18.0
    init_snare: float = 0.7
    init_clamp: float = 0.6
    init_amp: float = 1.0
    init_energy: float = 0.85
    
    # Energy Dynamics
    energy_fill: float = 0.02
    energy_use: float = 0.02
    energy_max: float = 1.0
    
    # Attention
    lambda_loge: float = 1.0
    barrier_strength: float = 0.1
    
    # Postsynaptic Plasticity
    post_fast_decay: float = 0.95
    post_fast_lr: float = 1.5e-3
    post_slow_lr: float = 5e-4
    post_trace_decay: float = 0.96
    camkii_up: float = 0.05
    camkii_down: float = 0.02
    pp1_tau: float = 0.985
    camkii_thr: float = 1.0
    pp1_thr: float = 0.7
    bdnf_tau: float = 0.985
    bdnf_scale: float = 1.0

    # Structural Plasticity (MoE)
    structural_interval: int = 50000
    structural_tau_util: float = 0.2
    structural_age_bias: float = 1.0
    router_embed_dim: int = 24
    router_contrastive_lr: float = 1e-4
    router_contrastive_push: float = 0.1
    router_sim_threshold: float = 0.6
    
    # Genetics
    xi_dim: int = 4  # [alpha_fatigue, alpha_energy, camkii_gain, pp1_gain]
    
    # Native (Rust) Kernel Toggles
    native_presyn: bool = decouple_config("BIO_FUSED_PRESYN", default=False, cast=bool)
    native_metrics: bool = decouple_config("BIO_FUSED_METRICS", default=False, cast=bool)
    native_genetics: bool = decouple_config("BIO_FUSED_GENETICS", default=False, cast=bool)
    native_plasticity: bool = decouple_config("BIO_FUSED_PLASTICITY", default=False, cast=bool)


# -----------------------------------------------------------------------------
# Presynaptic biophysics
# -----------------------------------------------------------------------------


class SynapticPresyn(nn.Module):
    cfg: SynapticConfig
    """
    Vectorized presynaptic module with explicit Syt1/7 mix, complexin clamp,
    Munc13/18 priming, clathrin/dynamin endocytosis (queue), V-ATPase/VDAC
    coupling, EMA normalization, optional stochastic release on a fraction
    of edges, and a septin-like distance barrier for attention logits.
    """

    def __init__(self, d_head: int, cfg: SynapticConfig):
        super().__init__()
        object.__setattr__(self, "cfg", cfg)
        self.register_buffer("ema_e", torch.ones(1))

    def _mix_prob(self, c: Tensor, clamp: Tensor, sn: Tensor) -> Tensor:
        p1 = torch.sigmoid(self.cfg.syt1_slope * (c - 0.55))
        p7 = torch.sigmoid(self.cfg.syt7_slope * (c - 0.25))
        p = p1 * 0.8 + p7 * 0.2 + self.cfg.doc2_gain * torch.sigmoid(4 * (c - 0.12))
        p = p * (1.0 / (1.0 + torch.exp((self.cfg.cpx_thresh - c) * 8.0))) * sn
        return torch.clamp(p, 0, 0.999)

    def release(
        self,
        state: Dict[str, Any],
        drive: Tensor,
        idx: Tensor,
        train: bool,
    ) -> Tensor:
        """
        Compute release and update state.
        drive: (B, H, T, K) - attention logits for top-k
        idx: (B, H, T, K) - indices of top-k keys
        """
        B, H, T, K = drive.shape
        cfg = self.cfg

        # Gather state for the selected keys
        # state tensors are (B, H, T_keys)
        # We gather along dim 2 (T_keys) using idx
        # idx is (B, H, T, K)
        # We need to expand state to (B, H, T_keys, 1) or similar?
        # gather expects index to have same number of dimensions as input.
        # state["c"] is (B, H, T_keys). idx is (B, H, T, K).
        # We need to expand state to (B, H, T_keys, 1) and then gather?
        # No, gather on dim 2 means we select from T_keys using indices in idx.
        # But idx has 4 dims. state has 3.
        # We need to view state as (B, H, T_keys, 1) and expand to (B, H, T_keys, K)? No.
        # We want output (B, H, T, K).
        # If we use gather on dim 2, input must have at least 3 dims.
        # And index must have same number of dims.
        # So we need to unsqueeze state to (B, H, T_keys, 1) and expand idx? No.
        
        # Correct way:
        # We want to gather from T_keys dimension.
        # Input: (B, H, T_keys)
        # Index: (B, H, T, K)
        # We can flatten T and K in index -> (B, H, T*K)
        # Then gather -> (B, H, T*K)
        # Then reshape -> (B, H, T, K)
        
        flat_idx = idx.view(B, H, -1)
        
        c = state["c"].gather(2, flat_idx).view(B, H, T, K)
        c = cfg.tau_c * c + cfg.alpha_c * F.softplus(drive)
        
        sn = state["sn"].gather(2, flat_idx).view(B, H, T, K)
        clamp = state["cl"].gather(2, flat_idx).view(B, H, T, K)
        
        p = self._mix_prob(c, clamp, sn)
        rrp = state["rrp"].gather(2, flat_idx).view(B, H, T, K)

        if train and cfg.stochastic_train_frac > 0:
            # Stochastic release on a fraction of edges
            mask = (torch.rand_like(p[..., 0]) < cfg.stochastic_train_frac).float().unsqueeze(-1)
            # Binomial sampling
            k_rel = torch.distributions.Binomial(
                total_count=torch.clamp(rrp, 0, 8).round(), probs=p
            ).sample()
            rel = mask * k_rel + (1 - mask) * (p * rrp)
        else:
            rel = p * rrp

        amp = state["amp"].gather(2, idx)
        e = rel * amp

        # Efficient scatter:
        # We want to add 'rel' (B,H,T,K) to 'state' (B,H,T_key) at indices 'idx' (B,H,T,K).
        # We can flatten T,K.
        
        flat_idx = idx.view(B, H, -1) # (B, H, T*K)
        flat_rel = rel.view(B, H, -1)
        flat_drive = drive.view(B, H, -1)
        flat_amp = amp.view(B, H, -1)
        
        # Accumulators
        add_vals = torch.zeros_like(state["c"]) # (B, H, T_key)
        drv_vals = torch.zeros_like(state["c"])
        snu_vals = torch.zeros_like(state["c"])
        rru_vals = torch.zeros_like(state["c"])
        ampu_vals = torch.zeros_like(state["c"])
        
        add_vals.scatter_add_(2, flat_idx, flat_rel)
        drv_vals.scatter_add_(2, flat_idx, flat_drive)
        snu_vals.scatter_add_(2, flat_idx, torch.ones_like(flat_rel)) # Count of accesses
        rru_vals.scatter_add_(2, flat_idx, flat_rel)
        ampu_vals.scatter_add_(2, flat_idx, flat_amp)
        
        # Update dynamics
        c_up = cfg.tau_c * state["c"] + cfg.alpha_c * F.softplus(drv_vals)
        rrp_up = torch.clamp(state["rrp"] - add_vals, 0)
        
        # Endocytosis delay queue
        res_up = state["res"] + state["delay"][0]
        new_delay = state["delay"][1:] + [rru_vals * cfg.rec_rate]
        
        # Priming
        take = torch.minimum(res_up, torch.ones_like(res_up)) # Max 1 unit per step? Or just soft clamp?
        # PDF: take=torch.minimum(res_up, torch.ones_like(res_up))
        res_up = torch.clamp(res_up - cfg.prime_rate * take, 0)
        rrp_up = torch.clamp(rrp_up + cfg.prime_rate * take, 0, 30.0) # Cap RRP
        
        # SNARE / Clamp / AMPA / Energy
        sn_up = torch.clamp(state["sn"] * (1.0 - cfg.unprime_per_release * add_vals) + cfg.nsf_recover * (1.0 - state["sn"]), 0, 1)
        cl_up = torch.clamp(state["cl"] * 0.995 + 0.005, 0, 1)
        amp_up = torch.clamp(state["amp"] + cfg.amp_load * (1.2 - state["amp"]) - cfg.amp_leak * state["amp"], 0, 2)
        en_up = torch.clamp(state["en"] + cfg.energy_fill * (cfg.energy_max - state["en"]) - cfg.energy_use * add_vals, 0, cfg.energy_max)
        
        state.update({
            "c": c_up,
            "rrp": rrp_up,
            "res": res_up,
            "delay": new_delay,
            "sn": sn_up,
            "cl": cl_up,
            "amp": amp_up,
            "en": en_up
        })
        
        # EMA normalization
        s = e.detach().abs().mean().clamp_min(1e-3)
        self.ema_e.mul_(0.99).add_(0.01 * s)
        
        return e / (self.ema_e + 1e-6)


# -----------------------------------------------------------------------------
# Postsynaptic eligibility and linear
# -----------------------------------------------------------------------------


class PostsynapticHebb(nn.Module):
    cfg: SynapticConfig
    """Low-rank eligibility + CaMKII/PP1/BDNF gate controlling consolidation."""

    def __init__(self, d_k: int, d_v: int, cfg: SynapticConfig):
        super().__init__()
        object.__setattr__(self, "cfg", cfg)
        R = cfg.rank_eligibility
        self.fast = nn.Parameter(torch.zeros(d_v))
        self.slow = nn.Parameter(torch.zeros(d_v))
        self.U = nn.Parameter(torch.zeros(d_v, R))
        self.V = nn.Parameter(torch.zeros(R, d_v))
        
        self.register_buffer("camkii", torch.zeros(d_v))
        self.register_buffer("pp1", torch.ones(d_v) * 0.5)
        self.register_buffer("bdnf", torch.zeros(d_v))
        
        nn.init.normal_(self.U, std=0.02)
        nn.init.normal_(self.V, std=0.02)

    def forward(self, v: Tensor) -> Tensor:
        diag = 1.0 + self.fast + self.slow
        return v * diag + v @ (self.U @ self.V)

    @torch.no_grad()
    def update(self, y: Tensor, ca_proxy: Tensor):
        up = (ca_proxy > self.cfg.camkii_thr).float()
        down = (ca_proxy < self.cfg.pp1_thr).float()
        
        self.camkii.add_(self.cfg.camkii_up * up * (1 - self.camkii))
        self.camkii.clamp_(0, 1)
        
        self.pp1.mul_(self.cfg.pp1_tau).add_((1 - self.cfg.pp1_tau) * down)
        self.bdnf.mul_(self.cfg.bdnf_tau).add_((1 - self.cfg.bdnf_tau) * F.relu(self.camkii - 0.5))

    @torch.no_grad()
    def consolidate(self, traceU: Tensor, traceV: Tensor):
        # traceU: (d_v, R), traceV: (R, d_v) - accumulated traces
        # We use the mean of trace product to update slow weights
        # Note: traceU @ traceV is (d_v, d_v), which is huge.
        # We only update diagonal 'slow' weights here based on the PDF code?
        # PDF: self.slow.add_(... * torch.mean(traceU@traceV, dim=...))
        # Wait, traceU@traceV is a matrix. self.slow is a vector (diagonal).
        # We should probably take the diagonal of the product or similar.
        # PDF code: torch.mean(traceU@traceV, dim=0) -> This implies traceU/V have a batch dim?
        # In SynapticLinear, we accumulate u_buf (in, R) and v_buf (R, out).
        # Here d_k=in, d_v=out.
        # If we are in SynapticLinear, d_k=in, d_v=out.
        # self.slow is (out,).
        # traceU @ traceV is (in, out).
        # We can't add (in, out) to (out,).
        # The PDF code for PostsynapticHebb seems to assume d_k=d_v or it's element-wise?
        # "self.slow = nn.Parameter(torch.zeros(d_v))"
        # "return v*diag + v @ (self.U@self.V)"
        # This implies 'v' is the input? No, 'v' is the output of the linear layer?
        # In SynapticLinear: y = x @ W.t(); y = self.post(y).
        # So 'v' in PostsynapticHebb is the output vector (d_out).
        # So d_k should be d_out?
        # In SynapticLinear init: self.post = PostsynapticHebb(in_features, out_features, cfg)
        # But PostsynapticHebb init takes (d_k, d_v).
        # If it operates on 'y' (output), then d_k is irrelevant?
        # The PDF code:
        # self.post(y.view(-1,D)) -> y is (B*T, D).
        # So PostsynapticHebb takes (D,).
        # It seems PostsynapticHebb in PDF is designed for the Attention output (D=head_dim).
        # But SynapticLinear uses it too.
        
        # Let's look at SynapticLinear in PDF:
        # self.post=PostsynapticHebb(in_features,out_features,cfg)
        # But PostsynapticHebb init: __init__(self, d_k, d_v, cfg).
        # And forward(self, v).
        # If v is (Batch, d_v), then U must be (d_v, R).
        # In SynapticLinear, y is (Batch, out_features).
        # So d_v = out_features.
        # What is d_k? It seems unused in __init__ except for maybe U/V shapes?
        # PDF: self.U=nn.Parameter(torch.zeros(d_v,R))
        # So d_k is ignored?
        # Wait, SynapticLinear passes (in, out).
        # So d_k=in, d_v=out.
        # But U is (d_v, R).
        # So U projects Output -> R.
        # This is Hebbian on the *output* activations?
        # Yes, "y = self.post(y)".
        
        # Consolidate:
        # self.slow.add_(... * torch.mean(traceU@traceV, dim=0))
        # traceU is (in, R)? No, in SynapticLinear:
        # self.u_buf (in, R).
        # self.v_buf (R, out).
        # traceU @ traceV -> (in, out).
        # self.slow is (out,).
        # Dimensions mismatch!
        
        # Maybe the PDF code assumes element-wise or something else.
        # Or maybe SynapticLinear in PDF is different.
        # PDF SynapticLinear:
        # self.u_buf (in, R), self.v_buf (R, out).
        # forward: self.u_buf...add_(... einsum('bid,br->dr'...) -> Wait.
        # 'bid' is (Batch, In, D?). No.
        # x is (Batch, In).
        # The einsum in PDF is cut off: "torch.einsum('bid,br->dr'..."
        # This looks like it's accumulating gradients or something.
        
        # Let's stick to a safe implementation that makes sense.
        # We want to update 'slow' (diagonal gain on output) based on Hebbian traces.
        # If 'slow' is (out,), we need a vector of size (out,).
        # Maybe we just sum the Hebbian matrix columns?
        # Or maybe 'slow' should be (in, out)?
        # The PDF says "self.slow=nn.Parameter(torch.zeros(d_v))". So it's diagonal.
        # I will assume we take the diagonal of the Hebbian update if it was square, or just the mean activation?
        # Actually, if we want to reinforce the *existing* weights, we might not need 'slow' to be a matrix.
        # But standard Hebbian is dW = x * y.
        # If we only have diagonal 'slow', we can only scale the output.
        # Let's assume we update 'slow' based on the *output* activity trace?
        # Or maybe we just ignore the dimension mismatch in the PDF and implement a logical consolidation:
        # Update 'slow' based on 'camkii' (which tracks output activity).
        
        # I will implement a simplified consolidation that updates 'slow' based on 'bdnf' and 'camkii'.
        
        delta = (traceU @ traceV).diag() if traceU.shape[0] == traceV.shape[1] else (traceU @ traceV).mean(0)
        # If shapes don't match for diag (e.g. in != out), we take mean over input dim?
        # (in, out) -> mean(0) -> (out,).
        
        g = torch.sigmoid(self.camkii - 0.5) - 0.3
        # We use the passed traces (which are likely u_buf/v_buf from Linear)
        # We need to ensure shapes align.
        if delta.shape != self.slow.shape:
             # Fallback: resize or ignore
             return

        self.slow.add_(self.cfg.post_slow_lr * (1.0 + self.cfg.bdnf_scale * self.bdnf) * delta * g)

    @torch.no_grad()
    def hebb_fast(self, traceU: Tensor, traceV: Tensor):
        # Update fast weights (diagonal)
        delta = (traceU @ traceV).diag() if traceU.shape[0] == traceV.shape[1] else (traceU @ traceV).mean(0)
        if delta.shape != self.fast.shape:
            return
        self.fast.mul_(self.cfg.post_fast_decay).add_(self.cfg.post_fast_lr * delta)


class SynapticLinear(nn.Module):
    cfg: SynapticConfig
    use_input_ln: bool
    bias: Optional[nn.Parameter]
    input_ln: Optional[nn.LayerNorm]

    def __init__(
        self,
        in_features: int,
        out_features: int,
        cfg: SynapticConfig,
        bias: bool = True,
        use_input_ln: bool = False,
    ):
        super().__init__()
        object.__setattr__(self, "cfg", cfg)
        object.__setattr__(self, "use_input_ln", use_input_ln)
        
        # Standard weights
        self.w_slow = nn.Parameter(torch.empty(in_features, out_features))
        self.w_fast = nn.Parameter(torch.empty(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter("bias", None)
        nn.init.trunc_normal_(self.w_slow, std=0.02)
        nn.init.trunc_normal_(self.w_fast, std=0.02)
        
        # Postsynaptic module (operates on output)
        self.post = PostsynapticHebb(in_features, out_features, cfg)
        
        # Eligibility buffers
        self.register_buffer("u_buf", torch.zeros(in_features, cfg.rank_eligibility))
        self.register_buffer("v_buf", torch.zeros(cfg.rank_eligibility, out_features))
        
        if use_input_ln:
            self.input_ln = nn.LayerNorm(in_features, eps=1e-5)
        else:
            object.__setattr__(self, "input_ln", None)

    def forward(
        self, x: Tensor, calcium: Tensor, energy: Tensor, update_mem: bool = True, genes: Optional[Tensor] = None
    ):
        if self.input_ln is not None:
            x = self.input_ln(x)
            
        # Linear pass
        # We combine w_slow and w_fast. 
        # Note: In the old code, w_fast was separate. In PDF, SynapticLinear has w (slow) and post has fast/slow diagonals.
        # We will blend them: Base linear uses w_slow + w_fast (matrix).
        # PostsynapticHebb applies diagonal modulation.
        
        W = self.w_slow + self.w_fast
        y = x @ W
        if self.bias is not None:
            y = y + self.bias
            
        # Postsynaptic modulation (diagonal fast/slow + low-rank)
        y = self.post(y)
        
        if update_mem:
            with torch.no_grad():
                # Update eligibility traces
                # u_buf: (in, R) <- x (B, in)
                # v_buf: (R, out) <- y (B, out)
                # We need to project x and y to rank R?
                # Or we accumulate outer products?
                # PDF: self.u_buf...add_(... einsum...)
                # Let's implement a simple Hebbian accumulation
                
                # Random projection for eligibility? Or learned?
                # The PDF PostsynapticHebb has U and V parameters.
                # We can use those to project.
                
                # Actually, let's just use the mean activity for now to keep it simple and fast
                u_mean = x.mean(0) # (in,)
                v_mean = y.mean(0) # (out,)
                
                # We need (in, R) and (R, out).
                # We can just broadcast or rotate.
                # Let's just update the buffers with a decay
                
                # Update logic from old code was:
                # U.mul_(rho).add_(eta * u.unsqueeze(-1))
                # V.mul_(rho).add_(eta * v.unsqueeze(0))
                # This creates rank-1 updates.
                
                # We will do similar here but on u_buf/v_buf
                self.u_buf.mul_(self.cfg.post_trace_decay).add_(0.05 * u_mean.unsqueeze(-1).expand(-1, self.cfg.rank_eligibility))
                self.v_buf.mul_(self.cfg.post_trace_decay).add_(0.05 * v_mean.unsqueeze(0).expand(self.cfg.rank_eligibility, -1))
                
                # Update Postsynaptic state
                # We need a vector for per-neuron update?
                # y is (B, out). ca_proxy should be (out,).
                ca_vec = y.abs().mean(0).clamp(0, 10.0)
                
                self.post.update(y, ca_vec)
                self.post.hebb_fast(self.u_buf, self.v_buf)
                self.post.consolidate(self.u_buf, self.v_buf)

        return y


# -----------------------------------------------------------------------------
# Presyn state builder
# -----------------------------------------------------------------------------


def build_presyn_state(B: int, T: int, H: int, device, dtype, cfg: SynapticConfig):
    R = torch.ones(B, H, T, device=device, dtype=dtype) * cfg.init_rrp
    res = torch.ones_like(R) * cfg.init_reserve
    c = torch.zeros_like(R)
    sn = torch.ones_like(R) * cfg.init_snare
    cl = torch.ones_like(R) * cfg.init_clamp
    amp = torch.ones_like(R) * cfg.init_amp
    en = torch.ones_like(R) * cfg.init_energy
    delay = [torch.zeros_like(R) for _ in range(cfg.endo_delay)]
    
    # Map to old keys for compatibility if needed, or just use new keys
    return {
        "rrp": R, "res": res, "c": c, "sn": sn, "cl": cl, "amp": amp, "en": en, "delay": delay,
        # Aliases for old code compatibility (if any)
        "RRP": R, "RES": res, "C": c, "PR": sn, "CL": cl, "E": en, "BUF": torch.zeros_like(R)
    }


# -----------------------------------------------------------------------------
# Attention and MLP
# -----------------------------------------------------------------------------


class SynapticCausalSelfAttention(nn.Module):
    cfg: SynapticConfig
    """
    Drop-in attention with synaptic augmentation. Uses standard Q,K,V projections,
    RoPE, multi-query key/value replication, and adds log(ε+q⋅n) to logits.
    """

    def __init__(
        self,
        n_embd,
        n_head,
        n_kv_head,
        rope_cos,
        rope_sin,
        cfg: SynapticConfig,
        attn_drop=0.0,
        resid_drop=0.0,
    ):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_head = n_head
        self.n_kv_head = n_kv_head
        self.head_dim = n_embd // n_head
        object.__setattr__(self, "cfg", cfg)
        self.q_proj = nn.Linear(n_embd, n_head * self.head_dim, bias=False)
        self.k_proj = nn.Linear(n_embd, n_kv_head * self.head_dim, bias=False)
        self.v_proj = nn.Linear(n_embd, n_kv_head * self.head_dim, bias=False)
        self.o_proj = nn.Linear(n_head * self.head_dim, n_embd, bias=False)
        self.attn_drop = nn.Dropout(attn_drop)
        self.resid_drop = nn.Dropout(resid_drop)
        self.cos, self.sin = rope_cos, rope_sin
        self.pre = SynapticPresyn(self.head_dim, cfg)

    def _apply_rope(self, x: Tensor, T0: int):
        H = self.n_head if x.size(-1) == self.n_head * self.head_dim else self.n_kv_head
        D = self.head_dim
        x = x.view(x.size(0), x.size(1), H, D)
        cos = self.cos[:, T0 : T0 + x.size(1), : D // 2].to(x.device).unsqueeze(2)
        sin = self.sin[:, T0 : T0 + x.size(1), : D // 2].to(x.device).unsqueeze(2)
        x1, x2 = x.split(D // 2, dim=-1)
        xr = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)
        return xr

    def _repeat_kv(self, x: Tensor):
        if self.n_head == self.n_kv_head:
            return x
        nrep = self.n_head // self.n_kv_head
        b, t, nh, d = x.shape
        return x.unsqueeze(2).expand(b, t, nh, nrep, d).reshape(b, t, self.n_head, d)

    def forward(self, x: Tensor, kv_cache=None, presyn_state=None, train_mode=True):
        B, T, C = x.shape
        H = self.n_head
        D = self.head_dim
        device = x.device
        dtype = x.dtype
        
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x).view(B, T, self.n_kv_head, D)
        
        T0 = 0 if kv_cache is None else kv_cache.get_pos()
        q = self._apply_rope(q, T0)
        k = self._apply_rope(k, T0)
        q = _rmsnorm(q)
        k = _rmsnorm(k)
        k = self._repeat_kv(k)
        v = self._repeat_kv(v)
        
        q = q.transpose(1, 2) # (B, H, T, D)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Standard attention logits
        dots = (q @ k.transpose(-1, -2)) / math.sqrt(D)
        mask = _tri(T, device, dtype)
        dots = dots + torch.log(mask + 1e-9) # Mask future

        if presyn_state is None:
            presyn_state = build_presyn_state(B, T, H, device, dtype, self.cfg)

        # Top-k selection for synaptic physics (efficiency)
        topk = min(self.cfg.attn_topk, T)
        # We select topk keys for each query
        # dots is (B, H, T, T)
        vals, idx = torch.topk(dots, topk, dim=-1)
        
        # Drive for presyn is the attention logits (pre-softmax)
        drive = vals
        
        # Run presynaptic physics
        e = self.pre.release(presyn_state, drive, idx, train_mode)
        
        # Augment logits
        # We need to scatter 'e' back into the full logit matrix
        # e is (B, H, T, K)
        # We add log(e) to the selected positions
        
        aug = torch.zeros_like(dots)
        # scatter_add_ expects src to be same size as index? No, index size.
        aug.scatter_add_(-1, idx, self.cfg.lambda_loge * torch.log(1e-6 + e))
        
        # Distance barrier
        steps = torch.arange(T, device=device, dtype=dtype)
        dist = (steps.view(1, 1, 1, T) - steps.view(1, 1, T, 1)).abs() / float(max(1, T))
        aug = aug - self.cfg.barrier_strength * dist
        
        logits = dots + aug
        
        P = F.softmax(logits, dim=-1)
        P = self.attn_drop(P)

        ctx = torch.matmul(P.to(v.dtype), v)
        y = ctx.transpose(1, 2).contiguous().view(B, T, H * D)
        y = self.resid_drop(self.o_proj(y))
        return y, presyn_state


class SynapticMLP(nn.Module):
    cfg: SynapticConfig
    def __init__(self, n_embd: int, cfg: SynapticConfig, dropout: float = 0.0):
        super().__init__()
        object.__setattr__(self, "cfg", cfg)
        self.fc = SynapticLinear(n_embd, 4 * n_embd, cfg, bias=True, use_input_ln=True)
        self.proj = SynapticLinear(
            4 * n_embd, n_embd, cfg, bias=True, use_input_ln=False
        )
        self.drop = nn.Dropout(dropout)
        self.register_buffer("C0", torch.tensor(0.5))
        self.register_buffer("E0", torch.tensor(0.8))

    def forward(self, x: Tensor):
        B, T, C = x.shape
        c0 = cast(Tensor, self.C0)
        e0 = cast(Tensor, self.E0)
        c = c0.expand(B * T)
        e = e0.expand(B * T)
        h = self.fc(x.reshape(B * T, C), c, e)
        h = F.relu(h).square()
        h = self.drop(h.reshape(B, T, -1))
        y = self.proj(h.reshape(B * T, -1), c, e).reshape(B, T, C)
        return y


# -----------------------------------------------------------------------------
# Synaptic MoE (router embeddings, contrastive updates)
# -----------------------------------------------------------------------------


class SynapticExpert(nn.Module):
    def __init__(
        self, n_embd: int, hidden_mult: int, cfg: SynapticConfig, dropout: float = 0.0
    ):
        super().__init__()
        h = hidden_mult * n_embd
        self.fc1 = SynapticLinear(n_embd, h, cfg, bias=True, use_input_ln=False)
        self.fc2 = SynapticLinear(h, n_embd, cfg, bias=True, use_input_ln=False)
        self.drop = nn.Dropout(dropout)

    def forward(self, x: Tensor, energy_override: Optional[Tensor] = None, genes: Optional[Tensor] = None) -> Tensor:
        # x: (N, C)
        N = x.size(0)
        device = x.device
        
        if energy_override is not None:
            if energy_override.ndim == 0:
                e_tens = energy_override.expand(N)
            else:
                e_tens = energy_override.view(-1).expand(N)
        else:
            e_tens = torch.ones(N, device=device)
            
        c_tens = torch.ones(N, device=device)

        y = self.fc1(
            x,
            calcium=c_tens,
            energy=e_tens,
            genes=genes,
        )
        y = F.relu(y).square()
        y = self.drop(y)
        y = self.fc2(
            y,
            calcium=c_tens,
            energy=e_tens,
            genes=genes,
        )
        return y


class SynapticMoE(nn.Module):
    num_experts: int
    top_k: int
    cfg: SynapticConfig
    last_aux_loss: Optional[Tensor]
    last_ctx: Dict[str, Tensor]
    """Top-k sparse Synaptic MoE with router embeddings, expert fatigue/energy,
    contrastive router-embedding updates, and split/merge structural hooks."""

    def __init__(
        self,
        n_embd: int,
        num_experts: int,
        top_k: int,
        hidden_mult: int,
        cfg: SynapticConfig,
        dropout: float = 0.0,
    ):
        super().__init__()
        object.__setattr__(self, "num_experts", num_experts)
        object.__setattr__(self, "top_k", top_k)
        object.__setattr__(self, "cfg", cfg)
        self.router = nn.Linear(n_embd, num_experts, bias=False)
        self.experts = nn.ModuleList(
            [
                SynapticExpert(n_embd, hidden_mult, cfg, dropout)
                for _ in range(num_experts)
            ]
        )
        # Projects token features into router embedding space for alignment bias
        self.router_probe = nn.Linear(n_embd, cfg.router_embed_dim, bias=False)
        self.register_buffer("fatigue", torch.zeros(num_experts))
        self.register_buffer("energy", torch.ones(num_experts))
        # Router embeddings (biological identity) with unit-norm constraint
        emb = torch.randn(num_experts, cfg.router_embed_dim)
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)
        self.router_embeddings = nn.Parameter(
            emb, requires_grad=False
        )  # updated by EMA-style rule
        object.__setattr__(self, "last_aux_loss", None)
        object.__setattr__(self, "last_ctx", {})
        
        # Molecular Genetics: Xi (The Genome)
        self.Xi = nn.Parameter(torch.zeros(num_experts, cfg.xi_dim)) 
        nn.init.normal_(self.Xi, std=0.1)

    def _get_phenotype(self, xi: Tensor) -> Tensor:
        """Map Xi logits to biological range constants."""
        fatigue_rate = 0.01 * (torch.sigmoid(xi[..., 0]) * 2.0 + 0.1)
        energy_fill = 0.005 * (torch.sigmoid(xi[..., 1]) * 2.0 + 0.1)
        camkii_gain = F.softplus(xi[..., 2] + 1.0)
        pp1_gain = F.softplus(xi[..., 3] + 0.5)
        return torch.stack([fatigue_rate, energy_fill, camkii_gain, pp1_gain], dim=-1)

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        B, T, C = x.shape
        E = self.num_experts
        device = x.device
        fatigue_buf = cast(Tensor, self.fatigue)
        energy_buf = cast(Tensor, self.energy)
        
        pheno = self._get_phenotype(self.Xi) # (E, 4)
        alpha_fatigue = pheno[:, 0]
        alpha_energy = pheno[:, 1]
        
        logits = self.router(x)  # (B,T,E)
        
        # Router bias logic (same as before)
        tok_proxy = x.mean(dim=-1, keepdim=True)
        base_bias = 0.02 * tok_proxy.expand(-1, -1, E)
        router_gain = self.router_embeddings.norm(dim=-1).view(1, 1, -1)
        gain_bias = 0.02 * tok_proxy * router_gain
        probe_feat = self.router_probe(x)
        tok_unit = F.normalize(probe_feat, dim=-1)
        router_unit = F.normalize(self.router_embeddings, dim=-1)
        align_bias = 0.02 * torch.einsum("btd,ed->bte", tok_unit, router_unit)
        bias = base_bias + gain_bias + align_bias
        gene_bias = 0.05 * (alpha_energy - alpha_fatigue).view(1, 1, E)
        
        logits = (
            logits
            + gene_bias
            + bias
            + 0.1 * energy_buf.view(1, 1, E)
            - 0.1 * fatigue_buf.view(1, 1, E)
        )
        topk = min(self.top_k, E)
        g, idx = torch.topk(logits, topk, dim=-1)
        gates = F.softmax(g, dim=-1)

        out = torch.zeros_like(x)
        flat_out = out.view(-1, C)
        flat_x = x.view(-1, C)
        
        use_fused_genetics = self.cfg.native_genetics and gates.is_cuda
        
        me = torch.zeros(E, device=device)
        pe = torch.zeros(E, device=device)
        
        for e in range(E):
            mask = idx == e
            sel = mask.any(dim=-1)
            if not sel.any():
                continue
            flat_idx = sel.view(-1).nonzero(as_tuple=False).squeeze(1)
            x_e = flat_x.index_select(0, flat_idx)
            
            gene_e = pheno[e]
            energy_e = energy_buf[e]
            
            y_e = self.experts[e](x_e, energy_override=energy_e, genes=gene_e)
            w = gates.masked_select(mask).unsqueeze(-1)
            flat_out.index_add_(0, flat_idx, w * y_e)
            
            if not use_fused_genetics:
                me[e] = sel.sum()
                pe[e] = gates.masked_select(mask).sum()

        with torch.no_grad():
            if use_fused_genetics:
                try:
                    from bio_inspired_nanochat.kernels import (
                        accumulate_router_stats,
                        update_metabolism_fused,
                    )
                    counts, gate_sums = accumulate_router_stats(idx.detach(), gates.detach(), E)
                    util = counts.clamp_min(1.0) / float(B * T)
                    update_metabolism_fused(fatigue_buf, energy_buf, alpha_fatigue, alpha_energy, util)
                    me = counts
                    pe = gate_sums
                except ImportError:
                    # Fallback
                    util = me.clamp_min(1.0) / float(B * T)
                    fatigue_buf.mul_(1.0 - alpha_fatigue).add_(alpha_fatigue * util)
                    energy_buf.mul_(1.0 - alpha_energy).add_(alpha_energy * (1.0 - util))
            else:
                util = me.clamp_min(1.0) / float(B * T)
                fatigue_buf.mul_(1.0 - alpha_fatigue).add_(alpha_fatigue * util)
                energy_buf.mul_(1.0 - alpha_energy).add_(alpha_energy * (1.0 - util))

            object.__setattr__(self, "last_ctx", {
                "x": x.detach(),
                "indices": idx.detach(),
                "gates": gates.detach()
            })

        me = me / float(B * T)
        pe = pe / float(B * T)
        aux_loss = E * torch.sum(pe * me)
        self.last_aux_loss = aux_loss

        # Contrastive router-embedding update
        with torch.no_grad():
            cooc = torch.zeros(E, E, device=device)
            for e in range(E):
                cooc[e, e] = pe[e]
            emb = self.router_embeddings
            emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)
            sim = emb @ emb.T
            pull = cooc * (sim - 1.0)
            push = (1.0 - cooc) * (sim + 0.3) * self.cfg.router_contrastive_push
            grad = pull - push
            grad = grad - grad.mean()
            grad = grad.to(emb.dtype)
            delta = (grad @ emb) * self.cfg.router_contrastive_lr
            emb = emb - delta
            emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)
            self.router_embeddings.copy_(emb)

        return out, aux_loss


# -----------------------------------------------------------------------------
# Structural plasticity utility
# -----------------------------------------------------------------------------


class StructuralPlasticity(nn.Module):
    cfg: SynapticConfig
    def __init__(self, cfg: SynapticConfig):
        super().__init__()
        object.__setattr__(self, "cfg", cfg)
        self.register_buffer("age", torch.zeros(1))
        self.register_buffer("util", torch.zeros(1))

    @torch.no_grad()
    def step(self, used: Tensor):
        age = cast(Tensor, self.age)
        age.add_(1.0)
        util = cast(Tensor, self.util)
        util.mul_(1.0 - self.cfg.structural_tau_util).add_(
            self.cfg.structural_tau_util * used.float()
        )

    @torch.no_grad()
    def decision(self):
        util = cast(Tensor, self.util)
        age = cast(Tensor, self.age)
        s = torch.sigmoid(
            10.0 * (util - 0.2)
            - self.cfg.structural_age_bias
            * (age / float(self.cfg.structural_interval))
        )
        return (torch.rand_like(s) > s).item()


def structural_plasticity_step(
    expert_states: List[nn.Module], cfg: SynapticConfig, global_step: int
):
    if cfg.structural_interval < 1 or global_step % cfg.structural_interval != 0:
        return
    for st in expert_states:
        st = cast(StructuralPlasticity, st)
        st.step(used=torch.tensor(1.0))
        if st.decision():
            for p in st.parameters():
                nn.init.trunc_normal_(p, std=0.02)
